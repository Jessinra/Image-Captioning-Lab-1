{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 150\n",
    "\n",
    "\n",
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"START \" + annot['caption']\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "train_captions = all_captions[:NUM_SAMPLES]\n",
    "train_img_paths = all_img_paths[:NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 150\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"xception\"\n",
    "\n",
    "\n",
    "def get_encoder(model_type=MODEL_TYPE):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.keras.applications.xception\n",
    "        cnn_model = tf.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "encoder, encoder_preprocessor = get_encoder(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = encoder_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "# Get unique images\n",
    "unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# Prepare dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 19.75\n"
     ]
    }
   ],
   "source": [
    "estimated_batch_count = NUM_SAMPLES / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:04,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed image (batch)\n",
    "for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "    # get context vector of batch images\n",
    "    batch_features = encoder(batch_imgs)\n",
    "    \n",
    "    # flatten 2D cnn result into 1D for RNN decoder input\n",
    "    # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    # Cache preprocessed image\n",
    "    for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "        image_path = image_path.numpy().decode(\"utf-8\")\n",
    "        np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"BERT\"\n",
    "VOCAB_SIZE = 5000  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(x) for x in texts]\n",
    "        token_ids = [self.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        \n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOKENIZER == \"BERT\" :\n",
    "\n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "else : \n",
    "    \n",
    "    # use default keras tokenizer\n",
    "    tokenizer = TokenizerWrapper(num_words=VOCAB_SIZE, oov_token=\"[UNK]\")\n",
    "    tokenizer.fit_on_texts(train_captions)    \n",
    "    tokenizer.word_index['[PAD]'] = 0\n",
    "    tokenizer.index_word[0] = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "MAX_LENGTH = None  # use <int> or None\n",
    "\n",
    "\n",
    "train_captions = pad_sequences(train_captions, maxlen=MAX_LENGTH, padding='post', truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def load_image_npy(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "from tensorflow.data import Dataset\n",
    "dataset = Dataset.from_tensor_slices((train_img_paths, train_captions))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          load_image_npy, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(NUM_SAMPLES / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 8, 100, 2048)\n",
    "# caption => (batch_size = 8, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 13 batches, (total : 104)\n",
      "eval : 2 batches, (total : 16)\n",
      "test : 4 batches, (total : 32 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, 64, output_dim)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, 64, embedding_dim)\n",
    "        hidden                        => (batch_size, hidden_size = ??)\n",
    "        \n",
    "        hidden_with_time_axis => (batch_size, 1, hidden_size)\n",
    "        score                 => (batch_size, 64, hidden_size)\n",
    "        attention_weights     => (batch_size, 64, 1)\n",
    "        context_vector (after sum) => (batch_size, hidden_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam()\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    real => (batch_size = 8, (1))\n",
    "    pred => (batch_size = 8, vocab_size)\n",
    "    mask => Tensor(\"Cast:0\", shape=(batch_size,), dtype=float32)\n",
    "    loss_ => Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(batch_size,), dtype=float32)\n",
    "\n",
    "    return => Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "\n",
    "WORD_EMBEDDING = \"BERT\"\n",
    "RNN_TYPE = \"LSTM\"\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=32, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=5000):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(vocab_size) # same size as vocab\n",
    "\n",
    "        # attention layer for image (CNN encoder output)\n",
    "        self.attention = BahdanauAttention(self.rnn_units)\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.bert_embedding.to('cuda')\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, x):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \n",
    "        x (Text tokens) => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            return self._bert_embedding(x)\n",
    "        else:\n",
    "            return self._default_embedding(x)\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x, output_layer=11):\n",
    "\n",
    "        # Format as torch Tensor\n",
    "        x = torch.as_tensor(x.numpy())\n",
    "        x = x.type(torch.LongTensor).to('cuda')\n",
    "        \n",
    "        # BERT's embedding\n",
    "        with torch.no_grad():\n",
    "            embedding , _ = self.bert_embedding(x)\n",
    "\n",
    "        # Revert back to tf.Tensor\n",
    "        x = embedding[output_layer].cpu().numpy()\n",
    "        x = tf.convert_to_tensor(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        return self.default_embedding(x)\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=0):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.combine_strategy == \"inject_init\":\n",
    "            initial_state = context_vector if curr_iter == 1 else None\n",
    "            output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "          \n",
    "        elif self.combine_strategy == \"inject_pre\":\n",
    "            x = context_vector if curr_iter == 1 else x\n",
    "            output, state = self.rnn_model(x)  \n",
    "            \n",
    "        elif self.combine_strategy == \"inject_par\":\n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            output, state = self.rnn_model(x)              \n",
    "\n",
    "        else: # merge (as default)\n",
    "            output, state = self.rnn_model(x)           \n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, x, context_vector, iteration):\n",
    "        \"\"\" \n",
    "\n",
    "        x : decoder input, also last word generated => (batch_size, 1)\n",
    "        context_vector : image's vector\n",
    "        \"\"\"\n",
    "\n",
    "        # x1 => (batch_size, 1, embedding_dim)\n",
    "        x1 = self.embedding(x)\n",
    "        \n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector, iteration)\n",
    "            \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "            \n",
    "        # x3 shape => (batch_size, 1, rnn_units = 32)\n",
    "        x3 = self.fc1(x2)\n",
    "\n",
    "        # x4 => (batch_size, rnn_units = 32)\n",
    "        x4 = tf.reshape(x3, (-1, x3.shape[2]))\n",
    "\n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        ## ====================================================\n",
    "        ## TODO: add PPLM framework here\n",
    "        ## feed-forward, find gradient, apply gradient, feed-forward again \n",
    "        ## ====================================================\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.rnn_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "\n",
    "\n",
    "def train_step(img_tensor, target):\n",
    "    \n",
    "    loss = 0\n",
    "    batch_size = target.shape[0]\n",
    "    \n",
    "    ## Get image context vector\n",
    "    features = encoder(img_tensor)\n",
    "    \n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    ## decoder_input == last word generated\n",
    "    decoder_input = tf.expand_dims(target[:, 0], 1)\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        features = encoder(img_tensor)\n",
    "        \n",
    "        for i in range(1, target.shape[1]):\n",
    "\n",
    "            context_vector, attention_weights = attention(features, hidden)\n",
    "            context_vector = tf.expand_dims(context_vector, 1)     \n",
    "            \n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden = decoder(decoder_input, context_vector, iteration=i)\n",
    "            \n",
    "            # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "            \n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables + attention.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 32\n",
    "IMAGE_FEATURE_DIM = 256\n",
    "WORD_EMBEDDING_DIM = 768 # BERT's embedding dim\n",
    "VOCAB_SIZE = len(tokenizer.vocab.keys()) # BERT's vocab size\n",
    "\n",
    "# WORD_EMBEDDING_DIM = 256\n",
    "# VOCAB_SIZE = 3000\n",
    "\n",
    "\n",
    "encoder = CNN_Encoder(\n",
    "    output_dim=IMAGE_FEATURE_DIM\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(units=UNITS)\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=\"LSTM\",\n",
    "    rnn_units=UNITS,\n",
    "    embedding_type=\"BERT\",\n",
    "    embedding_dim=WORD_EMBEDDING_DIM,  \n",
    "    combine_strategy=\"inject_par\", \n",
    "    combine_layer=\"concat\",\n",
    "    vocab_size=VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.6470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:13,  1.02s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 65.078896\n",
      "Time taken for 1 epoch 13.283867597579956 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 5.2919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:12,  1.00it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 57.532646\n",
      "Time taken for 1 epoch 12.95455813407898 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 0 Loss 4.3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:12,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 43.569706\n",
      "Time taken for 1 epoch 12.645180463790894 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset)):\n",
    "        \n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            \n",
    "        \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def eval_step(img_tensor, target):\n",
    "#     \"\"\"\n",
    "#     basically same as train_step, but doesn't apply gradient \n",
    "#     \"\"\"\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     # initializing the hidden state for each batch\n",
    "#     # because the captions are not related from image to image\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']] * target.shape[0], 1)\n",
    "    \n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         features = encoder(img_tensor)\n",
    "\n",
    "#         for i in range(1, target.shape[1]):\n",
    "            \n",
    "#             # passing the features through the decoder\n",
    "#             predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "\n",
    "#             loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "#             # using teacher forcing\n",
    "#             decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "#     total_loss = (loss / int(target.shape[1]))\n",
    "    \n",
    "#     return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Customized to enable multiple images, and incorporate PPLM framework\n",
    "\n",
    "\n",
    "# def custom_evaluate(image, supporting_images):\n",
    "    \n",
    "#     attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "#     # initializing the hidden state for decoder\n",
    "#     hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     # Extract image features\n",
    "#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "#     features = encoder(img_tensor_val)\n",
    "    \n",
    "    \n",
    "#     ## TODO: extract features from supporting images\n",
    "#     ## TODO: average those features into single context vector\n",
    "    \n",
    "    \n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     ## TODO: BERT doesn't have word_index\n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']], 0)\n",
    "#     result = []\n",
    "\n",
    "#     for i in range(max_length):\n",
    "#         \"\"\"\n",
    "#         Currently using inject method \n",
    "#         (image feature injected at every timestep)\n",
    "        \n",
    "#         TODO: test merge approach & pre, par inject\n",
    "#         \"\"\"\n",
    "        \n",
    "#         ## image feature inject's method\n",
    "#         predictions, hidden, attention_weights = decoder(decoder_input, features, hidden)\n",
    "        \n",
    "        \n",
    "#         ## TODO : apply PPLM here\n",
    "#         ## check loss (prediction - context vector of supporting images)\n",
    "#         ## apply gradient : hidden_state += diffrence(pred, supporting img vectors) (after n-iteration)\n",
    "#         ## re compute predictions\n",
    "        \n",
    "        \n",
    "#         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        \n",
    "        \n",
    "#         ## TODO: Allow using BERT tokenizer\n",
    "#         ## TODO: Revert predictions to index and words\n",
    "#         result.append(tokenizer.index_word[predicted_id])   # <<< return back id to text\n",
    "\n",
    "        \n",
    "#         if tokenizer.index_word[predicted_id] == '[END]':\n",
    "#             return result, attention_plot\n",
    "\n",
    "#         decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result), :]\n",
    "#     return result, attention_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
