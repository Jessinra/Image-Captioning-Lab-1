{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git add \"Keras.ipynb\"\n",
    "!git commit -m \"test pred using pplm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"rnn_units\": 256,\n",
    "    \"rnn_type\": \"LSTM\",\n",
    "    \"tokenizer\": \"BERT\",\n",
    "    \"word_embedding\": \"BERT\",\n",
    "    \"vocab_size\": 3000,\n",
    "    \"combine_strategy\": \"merge\",\n",
    "    \"combine_layer\": \"concat\",\n",
    "    \"image_context_size\": 256,\n",
    "    \"word_embedding_dim\": 256,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_size\": 5000,\n",
    "    \"use_mapping\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_caption_length\": 25, # use <int> or None\n",
    "    \"image_feature_extractor\": \"xception\",\n",
    "    \"epoch\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "\n",
    "# for TFBertModel\n",
    "PROXIES = {\n",
    "  \"http\": \"http://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "  \"https\": \"https://jessin:77332066@cache.itb.ac.id:8080\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"[CLS] \" + annot['caption'] + \" [SEP]\"\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "\n",
    "stopper = -1 if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]\n",
    "train_captions = all_captions[:stopper]\n",
    "train_img_paths = all_img_paths[:stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 5000\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = len(train_captions) if PARAMS[\"data_size\"] == \"all\" else PARAMS[\"data_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature_extractor(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.compat.v1.keras.applications.xception\n",
    "        cnn_model = tf.compat.v1.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "def get_image_feature_shape(model_type):\n",
    "    \n",
    "    if model_type == \"xception\":\n",
    "        return (100, 2048)\n",
    "    elif model_type == \"inception_v3\":\n",
    "        return (64, 2048)\n",
    "    else:\n",
    "        raise Exception (\"model unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = extractor_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_SHAPE = get_image_feature_shape(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor, extractor_preprocessor = get_image_feature_extractor(PARAMS[\"image_feature_extractor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature\n",
    "\n",
    "this step mostly skipped since the results are cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 157.25\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = PARAMS[\"batch_size\"]\n",
    "\n",
    "estimated_batch_count = DATA_SIZE / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique images\n",
    "# unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# # Prepare dataset\n",
    "# image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "# image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessed image (batch)\n",
    "\n",
    "# for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "#     # get context vector of batch images\n",
    "#     batch_features = extractor(batch_imgs)\n",
    "    \n",
    "#     # flatten 2D cnn result into 1D for RNN decoder input\n",
    "#     # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "#     # image_feature = 100 (Xception)\n",
    "#     # image_feature = 64 (Inception V3)\n",
    "#     batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "#     # Cache preprocessed image\n",
    "#     for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "#         image_path = image_path.numpy().decode(\"utf-8\")\n",
    "#         np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Caption dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0305 11:25:57.217504 139825070298944 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0305 11:25:57.218636 139825070298944 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = PARAMS[\"tokenizer\"]\n",
    "VOCAB_SIZE = PARAMS[\"vocab_size\"]  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "    \n",
    "    def use_custom_mapping(self, use_mapping=True, vocab_size=3000):\n",
    "        \n",
    "        self.use_mapping = use_mapping\n",
    "        self.cust_vocab_size = vocab_size\n",
    "        self.mapping_initialized = False\n",
    "\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into custom indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        bert_ids = [self.convert_tokens_to_ids(self.tokenize(x)) for x in tqdm(texts)]\n",
    "        \n",
    "        if not self.use_mapping:\n",
    "            return bert_ids\n",
    "        \n",
    "        if not self.mapping_initialized:\n",
    "            self._initialize_custom_mapping(bert_ids)\n",
    "            return [self._convert_bert_id_to_custom_id(x) for x in tqdm(bert_ids)]\n",
    "        \n",
    "        return bert_ids\n",
    "    \n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \n",
    "        bert_ids = super().convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            return self._convert_bert_id_to_custom_id(bert_ids)\n",
    "        else:\n",
    "            return bert_ids\n",
    "        \n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \n",
    "        if self.use_mapping and self.mapping_initialized:\n",
    "            bert_ids = self._convert_custom_id_to_bert_id(token_ids)\n",
    "        else:\n",
    "            bert_ids = token_ids\n",
    "            \n",
    "        bert_tokens = super().convert_ids_to_tokens(bert_ids)\n",
    "        return bert_tokens\n",
    "    \n",
    "    \n",
    "    def _initialize_custom_mapping(self, corpus_bert_ids):\n",
    "        \n",
    "        print(\"    > constructing custom mapping < \\n\")\n",
    "        self._build_occurence_table(corpus_bert_ids)\n",
    "        self._build_custom_mapping_table()\n",
    "        self.mapping_initialized = True\n",
    "        \n",
    "        \n",
    "    def _build_occurence_table(self, tokenized_captions):\n",
    "        \"\"\"\n",
    "        build dict of token frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        self.occurence_table = {}\n",
    "        for caption in tqdm(tokenized_captions):\n",
    "            for token in caption:\n",
    "                if token not in self.occurence_table:\n",
    "                    self.occurence_table[token] = 0\n",
    "                self.occurence_table[token] += 1\n",
    "                \n",
    "    \n",
    "    def _build_custom_mapping_table(self):\n",
    "        \n",
    "        _special_token = ['[UNK]', '[PAD]']\n",
    "        _actual_vocab_size = self.cust_vocab_size - len(_special_token)\n",
    "        \n",
    "        sorted_occurence = {k: v for k, v in sorted(\n",
    "            self.occurence_table.items(), reverse=True, key=lambda item: item[1]\n",
    "        )}\n",
    "        \n",
    "        used_tokens = sorted(list(sorted_occurence)[:_actual_vocab_size])\n",
    "        mapping_size = min(len(used_tokens), _actual_vocab_size)\n",
    "        \n",
    "        _bert_pad = 0\n",
    "        _bert_oov = 100\n",
    "        self._custom_pad = 0\n",
    "        self._custom_oov = mapping_size + 1\n",
    "        \n",
    "        self.bert_id_to_custom_id = {\n",
    "            _bert_pad: self._custom_pad, \n",
    "            _bert_oov: self._custom_oov\n",
    "        }\n",
    "        self.custom_id_to_bert_id = {\n",
    "            self._custom_pad: _bert_pad, \n",
    "            self._custom_oov: _bert_oov\n",
    "        }\n",
    "        \n",
    "        for i in range(0, mapping_size):\n",
    "            bert_token = used_tokens[i]\n",
    "            self.bert_id_to_custom_id[bert_token] = i + 1    \n",
    "            self.custom_id_to_bert_id[i + 1] = bert_token\n",
    "            \n",
    "        print(\"Vocab contains {0} / {1} unique tokens ({2:.2f} %)\".format(\n",
    "            len(used_tokens) + 2,\\\n",
    "            len(sorted_occurence),\\\n",
    "            (len(used_tokens) / len(sorted_occurence) * 100)\n",
    "        ))\n",
    "        \n",
    "        sorted_occurence_count = list(sorted_occurence.values())\n",
    "        used_tokens_count = sum(sorted_occurence_count[:_actual_vocab_size])\n",
    "        total_tokens_count = sum(sorted_occurence_count)\n",
    "        \n",
    "        print(\"Using {0} / {1} tokens available ({2:.2f} %)\".format(\n",
    "            used_tokens_count,\\\n",
    "            total_tokens_count,\\\n",
    "            (used_tokens_count / total_tokens_count * 100)\n",
    "        ))        \n",
    "        \n",
    "    def _convert_bert_id_to_custom_id(self, token_ids):\n",
    "        return [self.bert_id_to_custom_id[x] if x in self.bert_id_to_custom_id else self._custom_oov for x in token_ids]\n",
    "    \n",
    "    def _convert_custom_id_to_bert_id(self, token_ids):\n",
    "        return [self.custom_id_to_bert_id[x] for x in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type, use_mapping, vocab_size):\n",
    "    \n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    if tokenizer_type == \"BERT\" :\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "        tokenizer.use_custom_mapping(use_mapping, vocab_size)\n",
    "\n",
    "    # use default keras tokenizer\n",
    "    else : \n",
    "        tokenizer = TokenizerWrapper(num_words=vocab_size, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0305 11:25:59.282871 139825070298944 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0305 11:26:00.449322 139825070298944 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/m13516112/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "caption_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=False,\n",
    "    vocab_size=0\n",
    ")\n",
    "\n",
    "target_tokenizer = get_tokenizer(\n",
    "    tokenizer_type=PARAMS[\"tokenizer\"],\n",
    "    use_mapping=PARAMS[\"use_mapping\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3104.86it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3202.96it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 245043.06it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 303174.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > constructing custom mapping < \n",
      "\n",
      "Vocab contains 3000 / 3584 unique tokens (83.65 %)\n",
      "Using 68179 / 68765 tokens available (99.15 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_tokens = caption_tokenizer.texts_to_sequences(train_captions)\n",
    "target_tokens = target_tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parallel dataset\n",
    "\n",
    "prepare captions and target to support parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parallel_dataset(img_paths, caption_tokens, target_tokens):\n",
    "    \n",
    "    dataset_img_paths = []\n",
    "    dataset_captions = []\n",
    "    dataset_target = []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_img_paths))):\n",
    "        img = img_paths[i]\n",
    "        cap = caption_tokens[i]\n",
    "        tar = target_tokens[i]\n",
    "\n",
    "        for j in range(1, len(cap)):\n",
    "            dataset_img_paths.append(img)\n",
    "            dataset_captions.append(cap[:j])\n",
    "            dataset_target.append(tar[j])\n",
    "\n",
    "    dataset_captions = pad_sequences(dataset_captions, maxlen=PARAMS[\"max_caption_length\"], padding='post')\n",
    "    \n",
    "    return dataset_img_paths, dataset_captions, dataset_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 21833.26it/s]\n"
     ]
    }
   ],
   "source": [
    "parallel_img_paths, parallel_captions, parallel_target = preprocess_parallel_dataset(train_img_paths, caption_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = parallel_captions.shape[0]\n",
    "MAX_CAPTION_LENGTH = parallel_captions.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_dataset(img_name, caption, target):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((parallel_img_paths, parallel_captions, parallel_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "          load_dataset, [item1, item2, item3], [tf.float32, tf.int32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(DATA_SIZE / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, image_feature = 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1395 batches, (total : 44640)\n",
      "eval : 298 batches, (total : 9536)\n",
      "test : 300 batches, (total : 9600 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=PARAMS[\"learning_rate\"])\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real  => (batch_size,)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    # create mask to filter out padding token \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    # Ignore loss_ if real token is padding\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Get mean of curren batch's loss (somewhat batch norm)\n",
    "    result_loss = tf.reduce_mean(loss_)\n",
    "    \n",
    "    return result_loss\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    loss_  => (batch_size, 1)\n",
    "    mask   => (batch_size, 1)  : indicate is padding or not\n",
    "\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        # x => (batch_size, 100, 2048)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, image_feature_size, image_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, rnn_units)\n",
    "        score                      => (batch_size, img_feature_size, rnn_units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Masking, LSTM, GRU\n",
    "from transformers import TFBertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=256, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=3000, batch_size=32):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "        \n",
    "        self.mask = Masking(mask_value=0, dtype=\"int32\")\n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size) # same size as vocab\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size) # same size as vocab\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = TFBertModel.from_pretrained('bert-base-uncased', proxies=PROXIES)\n",
    "            self.bert_embedding.trainable = False\n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(\n",
    "                input_dim=self.vocab_size, \n",
    "                output_dim=self.embedding_dim, \n",
    "                mask_zero=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, tokens, as_sentence=False):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            embedding, sentence_embedding = self._bert_embedding(tokens)\n",
    "        \n",
    "        else:\n",
    "            embedding = self._default_embedding(tokens)\n",
    "            sentence_embedding = tf.reduce_mean(embedding, 1)\n",
    "            \n",
    "\n",
    "        if as_sentence:\n",
    "            # embedding => (batch_size, embedding_dim)\n",
    "            return sentence_embedding\n",
    "        \n",
    "        else:\n",
    "            # embedding => (batch_size, tokens_length, embedding_dim)\n",
    "            return embedding\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        embedding (tokens)   => (batch_size, tokens_length, embedding_dim)\n",
    "        embedding (sentence) => (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x):\n",
    "        \n",
    "        # mask out attention from padding\n",
    "        # mask => (batch_size, sequence_length)\n",
    "        attention_mask = x == 0\n",
    "        \n",
    "        # hidden_states => (batch_size, sequence_length, embedding_size)\n",
    "        # sentence_embedding => (batch_size, embedding_size)\n",
    "        hidden_states, sentence_embedding = self.bert_embedding(inputs=x, attention_mask=attention_mask)\n",
    "        return hidden_states, sentence_embedding\n",
    "    \n",
    "        \"\"\"\n",
    "        hidden states contains hidden state for each word\n",
    "        sentence_embedding is general embedding for whole sentences\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        \n",
    "        # embedding => (batch_size, sequence_length, embedding_size)\n",
    "        embedding = self.default_embedding(x)\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=-1):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \n",
    "        No longer support inject_init & inject_pre, since training become fully parallel\n",
    "        \"\"\"\n",
    "        \n",
    "#         if self.combine_strategy == \"inject_init\":\n",
    "#             initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "#             output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "#         elif self.combine_strategy == \"inject_pre\":\n",
    "#             x = context_vector if curr_iter == 1 else x\n",
    "#             output, state = self.rnn_model(x)  \n",
    "            \n",
    "        if self.combine_strategy == \"inject_par\":\n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "                        \n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x)              \n",
    "            \n",
    "        else: # merge (as default)\n",
    "            \n",
    "            # output => (batch_size, sequence_len, rnn_unit)\n",
    "            output, state = self.rnn_model(x)           \n",
    "            \n",
    "            context_vector = tf.expand_dims(context_vector, 1)\n",
    "            context_vector = tf.tile(context_vector, [1, MAX_CAPTION_LENGTH, 1])\n",
    "            \n",
    "            # output => (batch_size, rnn_unit)\n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "            \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "        \"\"\"\n",
    "        output => (batch_size, rnn_size)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input, context_vector):\n",
    "        \"\"\" \n",
    "        decoder_input  : last predicted word => (batch_size, 1)\n",
    "        context_vector : image's vector      => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Mask out padding (0)\n",
    "        decoder_input = self.mask(decoder_input)\n",
    "        \n",
    "        # x1 => (batch_size, input_sentence_len, embedding_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "        \n",
    "        # x2 (concat) => (batch_size, embedding_dim + image_context_size)\n",
    "        # x2 (add) => (batch_size, embedding_dim)\n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector)\n",
    "        \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "        \n",
    "        # x3 => (batch_size, sequence_len, rnn_units)\n",
    "        x3 = self.fc1(x2)   # how important is every sequence \n",
    "        \n",
    "        # x4 => (batch_size, sequence_len * rnn_units) || was (batch_size * sequence_len=1, rnn_units)\n",
    "        x4 = tf.reshape(x3, (x3.shape[0], -1))\n",
    "\n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "    \n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \n",
    "        if batch_size is not None:\n",
    "            return tf.zeros((batch_size, self.rnn_units))\n",
    "        \n",
    "        return tf.zeros((self.batch_size, self.rnn_units))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn_units': 256,\n",
       " 'rnn_type': 'LSTM',\n",
       " 'tokenizer': 'BERT',\n",
       " 'word_embedding': 'BERT',\n",
       " 'vocab_size': 3000,\n",
       " 'combine_strategy': 'merge',\n",
       " 'combine_layer': 'concat',\n",
       " 'image_context_size': 256,\n",
       " 'word_embedding_dim': 256,\n",
       " 'batch_size': 32,\n",
       " 'data_size': 5000,\n",
       " 'use_mapping': True,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_caption_length': 25,\n",
       " 'image_feature_extractor': 'xception',\n",
       " 'epoch': 20}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0305 11:26:08.473015 139825070298944 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/m13516112/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0305 11:26:08.474699 139825070298944 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0305 11:26:09.657149 139825070298944 modeling_tf_utils.py:333] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-tf_model.h5 from cache at /home/m13516112/.cache/torch/transformers/d667df51ec24c20190f01fb4c20a21debc4c4fc12f7e2f5441ac0a99690e3ee9.4733ec82e81d40e9cf5fd04556267d8958fb150e9339390fc64206b7e5a79c83.h5\n",
      "I0305 11:26:13.794650 139825070298944 modeling_tf_utils.py:375] Layers from pretrained model not used in TFBertModel: ['nsp___cls', 'mlm___cls']\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder(\n",
    "    output_dim=PARAMS[\"image_context_size\"]\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=PARAMS[\"rnn_type\"], \n",
    "    rnn_units=PARAMS[\"rnn_units\"],\n",
    "    embedding_type=PARAMS[\"word_embedding\"], \n",
    "    embedding_dim=PARAMS[\"word_embedding_dim\"],  \n",
    "    combine_strategy=PARAMS[\"combine_strategy\"], \n",
    "    combine_layer=PARAMS[\"combine_layer\"],\n",
    "    vocab_size=PARAMS[\"vocab_size\"],\n",
    "    batch_size=PARAMS[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_CONTEXT_SIZE == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_CONTEXT_SIZE == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default feed forward function\n",
    "\n",
    "@tf.function\n",
    "def feed_forward(img_tensor, caption):\n",
    "    \"\"\"\n",
    "    img_tensor => (batch_size, image_feature_size, 2048)\n",
    "    caption => (batch_size, max_caption_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    # note : used to be decoder hidden state, \n",
    "    # but changed into current word embedding for paralel training\n",
    "    \n",
    "    # hidden => (batch_size, embedding_size)\n",
    "    hidden = decoder.embedding(caption, as_sentence=True)\n",
    "\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    # context_vector => (batch_size, image_context_size)\n",
    "    context_vector, _ = attention(features, hidden)\n",
    "\n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions, _ = decoder(caption, context_vector)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, caption, target):\n",
    "    \"\"\"\n",
    "    target => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training model\n",
    "    with tf.GradientTape() as gradient_tape:\n",
    "        \n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions = feed_forward(img_tensor, caption)\n",
    "        \n",
    "        # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        \n",
    "    # Apply gradient\n",
    "    trainable_variables = encoder.trainable_variables + \\\n",
    "                          decoder.trainable_variables + \\\n",
    "                          attention.trainable_variables\n",
    "    \n",
    "    gradients = gradient_tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_step(img_tensor, caption, target):\n",
    "    \n",
    "    # predictions => (batch_size, vocab_size)\n",
    "    predictions = feed_forward(img_tensor, caption)\n",
    "    \n",
    "    # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "    loss = loss_function(target, predictions)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def choose_predicted_id(predictions, strategy=\"max\", sampling_k=10):\n",
    "    \"\"\"\n",
    "    predictions : encoder word prediction => (batch_size, vocab_size)\n",
    "    strategy    : how to choose word [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sampling method (categorical dist)\n",
    "    if strategy == \"sample\":\n",
    "        \n",
    "        # sampled_proba & sampled_ids => (batch_size, sampling_k)\n",
    "        sampled_proba, sampled_ids = tf.math.top_k(predictions, sampling_k)\n",
    "        \n",
    "        # chosen_sampled_col => (batch_size, )\n",
    "        chosen_sampled_col = tf.squeeze(tf.random.categorical(sampled_proba, 1))\n",
    "        \n",
    "        # create row idx to zip with chosen_sampled_col\n",
    "        row_idx = tf.range(predictions.shape[0], dtype=chosen_sampled_col.dtype)\n",
    "        row_col_idx = tf.stack([row_idx, chosen_sampled_col], axis=1)\n",
    "        \n",
    "        # predicted_ids => (batch_size, )\n",
    "        predicted_ids = tf.gather_nd(sampled_ids, row_col_idx)\n",
    "\n",
    "    # Max index method\n",
    "    else:\n",
    "        predicted_ids = tf.argmax(predictions, 1)\n",
    "    \n",
    "    # predicted_ids => (batch_size, )\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images_paths):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [load_image(x)[0] for x in images_paths]\n",
    "    \n",
    "    # x => (batch_size, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, 10, 10, 2048)\n",
    "    x = extractor(x)\n",
    "    \n",
    "    # x  => (batch_size, img_feature_size, 2048)\n",
    "    x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_supporting_features(images_paths, strategy=\"mean\"):\n",
    "    \"\"\"\n",
    "    images_paths => (batch_size, img_count, 1)\n",
    "    strategy : strategy to aggregate multiple supporting image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract images features\n",
    "    images = [[load_image(x)[0] for x in images_set] for images_set in images_paths]\n",
    "    \n",
    "    # x => (batch_size, img_count, 299, 299, 3)\n",
    "    x = tf.convert_to_tensor(images)\n",
    "    \n",
    "    # x => (batch_size, img_count, 10, 10, 2048)\n",
    "    x = [extractor(image_set) for image_set in x]\n",
    "    \n",
    "    # features => (batch_size, img_count, img_feature_size, image_context_size)\n",
    "    features = encoder(x)\n",
    "    \n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    if strategy == \"logsumexp\":\n",
    "        features = tf.reduce_logsumexp(features, 1)\n",
    "    elif strategy == \"max\":\n",
    "        features = tf.reduce_max(features, 1)\n",
    "    elif strategy == \"min\":\n",
    "        features = tf.reduce_min(features, 1)\n",
    "    else:\n",
    "        features = tf.reduce_mean(features, 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_indices(support_text):\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(0, len(support_text)):\n",
    "        context_token = target_tokenizer.tokenize(support_text[i])\n",
    "        context_token_id = target_tokenizer.convert_tokens_to_ids(context_token)\n",
    "        context_token_id = set(context_token_id)\n",
    "        context_token_id.discard(0)\n",
    "        for x in sorted(context_token_id):\n",
    "            indices.append([i, x])\n",
    "\n",
    "    # return => (word_count, 2)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_supporting_text_vector(support_text, vocab_size):\n",
    "    \"\"\"\n",
    "    support_text : list of text describing main image context => (batch_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(support_text)\n",
    "    \n",
    "    # indices => ( sum(batch_size * ?word_count), 2)\n",
    "    indices = get_one_hot_indices(support_text)\n",
    "    values = tf.ones(len(indices))\n",
    "    sparse_one_hot = tf.sparse.SparseTensor(indices, values, dense_shape=[batch_size, vocab_size])\n",
    "    \n",
    "    # sparse_one_hot => (batch_size, vocab_size)\n",
    "    return sparse_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate(images_paths,\n",
    "                    support_text=None,\n",
    "                    support_imgs=None, \n",
    "                    support_aggregate_strategy=\"mean\",\n",
    "                    pplm_iteration=3,\n",
    "                    pplm_weight=0.03,\n",
    "                    pplm_gm_weight=0.8,\n",
    "                    choose_word_strategy=\"sample\",\n",
    "                   ):\n",
    "    \n",
    "    \"\"\"\n",
    "    images_paths : list of image_path                           => (batch_size, 1)\n",
    "    support_text : list of text describing main image context   => (batch_size)\n",
    "    support_imgs : list of list of image_path                   => (batch_size, image_count, 1)\n",
    "    support_aggregate_strategy : how to aggregate support image [\"logsumexp\", \"mean\", \"min\", \"max\"]\n",
    "    pplm_iteration : number of pplm step done for every decoding step\n",
    "    pplm_weight    : weight of pplm loss\n",
    "    pplm_gm_weight : geometric mean fusion weight (0 means use only original prediction, 1 means use only pplm prediction)\n",
    "    choose_word_strategy : how to choose word from prediction distribution [\"sample\", \"max\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(images_paths)\n",
    "    \n",
    "    # initialize captions placeholder\n",
    "    start_token = caption_tokenizer.convert_tokens_to_ids(['[CLS]']) # use bert id's not custom id\n",
    "    result_captions = tf.tile(tf.expand_dims(start_token, 1), [batch_size, 1])\n",
    "    attention_plot = tf.reshape([], shape=(batch_size, 0, IMAGE_FEATURE_SHAPE[0]))\n",
    "    \n",
    "    # Extract features from main images\n",
    "    # features => (batch_size, img_feature_size, image_context_size)\n",
    "    features = get_image_features(images_paths)\n",
    "    \n",
    "    \n",
    "    if support_text is not None:\n",
    "        # support_text_vector => (batch_size, vocab_size)\n",
    "        support_text_vector = get_supporting_text_vector(support_text, decoder.vocab_size) \n",
    "    else:\n",
    "        # set all pplm related variable to 0\n",
    "        pplm_iteration = 0\n",
    "        pplm_weight = 0\n",
    "        pplm_gm_weight = 0\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(MAX_CAPTION_LENGTH)):\n",
    "        \n",
    "        # decoder_input => (batch_size, ~MAX_CAPTION_LENGTH)\n",
    "        caption = pad_sequences(result_captions, maxlen=MAX_CAPTION_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # hidden => (batch_size, embedding_size)\n",
    "        hidden = decoder.embedding(caption, as_sentence=True)  ## BOTTLE\n",
    "\n",
    "        # context_vector => (batch_size, image_context_size)\n",
    "        # attention_weights => (batch_size, img_feature_size, 1)\n",
    "        context_vector, attention_weights = attention(features, hidden)\n",
    "\n",
    "        # predictions => (batch_size, vocab_size)\n",
    "        predictions, _ = decoder(caption, context_vector)  \n",
    "        \n",
    "        # ======================== PPLM section ========================\n",
    "        ori_prediction = predictions\n",
    "        \n",
    "        curr_pertubation = tf.Variable(tf.zeros((batch_size, decoder.embedding_dim)), name=\"curr_pertubation\", trainable=True)\n",
    "        \n",
    "        \n",
    "        for j in range(pplm_iteration):\n",
    "            \n",
    "            with tf.GradientTape() as pplm_tape: \n",
    "                \n",
    "                hidden += curr_pertubation\n",
    "                \n",
    "                context_vector, attention_weights = attention(features, hidden)\n",
    "                predictions, _ = decoder(caption, context_vector)\n",
    "                pplm_loss = pplm_loss_function(support_text_vector, predictions, pplm_weight=pplm_weight)\n",
    "                \n",
    "            \"\"\"\n",
    "            most impactfull layer to train = last dense layer\n",
    "            \"\"\"\n",
    "            \n",
    "            trainable_variables = [curr_pertubation]\n",
    "            gradients = pplm_tape.gradient(pplm_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "            \n",
    "            predictions, _ = decoder(caption, context_vector)\n",
    "        \n",
    "        # fuse final pplm_prediction and original prediction\n",
    "        fused_predictions = (predictions * pplm_gm_weight) + (ori_prediction * (1 - pplm_gm_weight)) \n",
    "        \n",
    "        # predicted_ids => (batch_size,)\n",
    "        predicted_ids = choose_predicted_id(fused_predictions, strategy=choose_word_strategy)\n",
    "        \n",
    "        # convert custom id mapping to bert's id\n",
    "        # predicted_bert_ids => (batch_size, 1)\n",
    "        predicted_bert_ids = target_tokenizer._convert_custom_id_to_bert_id(predicted_ids.numpy())\n",
    "        predicted_bert_ids = tf.expand_dims(predicted_bert_ids, 1)\n",
    "        \n",
    "        # store result\n",
    "        result_captions = tf.concat([result_captions, predicted_bert_ids], axis=1)\n",
    "        \n",
    "        # attention_weights => (batch_size, 1, img_feature_size)\n",
    "        attention_weights = tf.reshape(attention_weights, shape=(batch_size, 1, -1))\n",
    "        \n",
    "        # assign attention weights to respective generated word\n",
    "        # attention_plot => (batch_size, ~max_caption_len, feature_size)\n",
    "        attention_plot = tf.concat([attention_plot, attention_weights], axis=1)\n",
    "        \n",
    "    # remove start token & revert to tokens\n",
    "    result_captions = [caption_tokenizer.convert_ids_to_tokens(x[1:]) for x in result_captions]\n",
    "    \n",
    "    return result_captions, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def pplm_loss_function(real, pred, pplm_weight=0.03):\n",
    "    \"\"\"\n",
    "    real  => (batch_size, vocab_size)\n",
    "    pred  => (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "\n",
    "#     mm = tf.sparse.sparse_dense_matmul(real, tf.transpose(pred))\n",
    "#     mm = tf.reduce_sum(tf.abs(mm), 1)\n",
    "#     loss = tf.reduce_sum(mm, 0)\n",
    "    \n",
    "    real = tf.sparse.to_dense(real, default_value=0)\n",
    "    pplm_loss = CategoricalCrossentropy(from_logits=True)\n",
    "    loss = pplm_loss(real, pred, pplm_weight)\n",
    "    \n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "    \"\"\"\n",
    "    return => (1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, img_feature_size, image_context_size)\n",
    "        hidden                        => (batch_size, embedding_size)\n",
    "        \n",
    "        note : \n",
    "        img_feature_size ==  64 for Inception V3,\n",
    "        img_feature_size == 100 for Xception,\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        _w1 = self.W1(features)\n",
    "        _w2 = self.W2(hidden_with_time_axis)\n",
    "        score = tf.nn.tanh(_w1 + _w2)\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_with_time_axis      => (batch_size, 1, rnn_units)\n",
    "        score                      => (batch_size, img_feature_size, rnn_units)\n",
    "        attention_weights          => (batch_size, img_feature_size, 1)\n",
    "        context_vector (after sum) => (batch_size, img_context_size)\n",
    "        \"\"\"\n",
    "        \n",
    "attention = BahdanauAttention(\n",
    "    units=PARAMS[\"rnn_units\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [06:27,  3.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f28d8529ba8>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "    batch_loss = train_step(img_tensor, captions, target)\n",
    "    loss.append(batch_loss.numpy())\n",
    "    \n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-5981adc29e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train/{}\".format(str(datetime.now())[:-10])\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           attention=attention,\n",
    "                           optimizer=optimizer\n",
    "                          )\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------\")\n",
    "print(checkpoint_path)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(checkpoint_path)\n",
    "with open(checkpoint_path + \"/config.txt\", \"w\") as f:\n",
    "    f.write(str(PARAMS))\n",
    "    \n",
    "log_file = open(checkpoint_path + \"/log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(start_epoch, PARAMS[\"epoch\"]):\n",
    "    \n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    batch = 1\n",
    "\n",
    "    for img_tensor, captions, target in tqdm(train_dataset):\n",
    "        \n",
    "        batch_loss = train_step(img_tensor, captions, target)\n",
    "        loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "\n",
    "            with open(checkpoint_path + \"/log.txt\", \"a\") as f:\n",
    "                log_message = ' {} Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    checkpoint_path, epoch + 1, batch, loss.numpy())\n",
    "                f.write(str(log_message + \"\\n\"))\n",
    "                print(log_message)   \n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(batch_loss.numpy())\n",
    "\n",
    "#     ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = []\n",
    "\n",
    "# for (img_tensor, captions, target) in tqdm(train_dataset):\n",
    "#     batch_loss = train_step(img_tensor, captions, target)\n",
    "#     loss.append(batch_loss.numpy())\n",
    "    \n",
    "# plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = all_img_paths[:4]\n",
    "# text = [\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "#     \"butter cream cheese blue cheese cottage cheese goats cheese crème fraîche eggs free range eggs margarine milk full-fat milk semi-skimmed milk skimmed milk sour cream yoghurt\",\n",
    "# ]\n",
    "\n",
    "# result, attention_plot = custom_evaluate(images, support_text=text, pplm_iteration=5, pplm_weight=1)\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
