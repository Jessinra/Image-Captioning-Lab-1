{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 300\n",
    "\n",
    "\n",
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"START \" + annot['caption']\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "train_captions = all_captions[:NUM_SAMPLES]\n",
    "train_img_paths = all_img_paths[:NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 300\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"xception\"\n",
    "\n",
    "\n",
    "def get_encoder(model_type=MODEL_TYPE):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.keras.applications.xception\n",
    "        cnn_model = tf.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "encoder, encoder_preprocessor = get_encoder(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = encoder_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "# Get unique images\n",
    "unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# Prepare dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 38.5\n"
     ]
    }
   ],
   "source": [
    "estimated_batch_count = NUM_SAMPLES / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:08,  4.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed image (batch)\n",
    "for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "    # get context vector of batch images\n",
    "    batch_features = encoder(batch_imgs)\n",
    "    \n",
    "    # flatten 2D cnn result into 1D for RNN decoder input\n",
    "    # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    # Cache preprocessed image\n",
    "    for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "        image_path = image_path.numpy().decode(\"utf-8\")\n",
    "        np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"BERT\"\n",
    "VOCAB_SIZE = 5000  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(x) for x in texts]\n",
    "        token_ids = [self.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        \n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOKENIZER == \"BERT\" :\n",
    "\n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "else : \n",
    "    \n",
    "    # use default keras tokenizer\n",
    "    tokenizer = TokenizerWrapper(num_words=VOCAB_SIZE, oov_token=\"[UNK]\")\n",
    "    tokenizer.fit_on_texts(train_captions)    \n",
    "    tokenizer.word_index['[PAD]'] = 0\n",
    "    tokenizer.index_word[0] = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "MAX_LENGTH = None  # use <int> or None\n",
    "\n",
    "\n",
    "train_captions = pad_sequences(train_captions, maxlen=MAX_LENGTH, padding='post', truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def load_image_npy(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "from tensorflow.data import Dataset\n",
    "dataset = Dataset.from_tensor_slices((train_img_paths, train_captions))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          load_image_npy, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(NUM_SAMPLES / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 8, 100, 2048)\n",
    "# caption => (batch_size = 8, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 26 batches, (total : 208)\n",
      "eval : 5 batches, (total : 40)\n",
      "test : 7 batches, (total : 56 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, 64, output_dim)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, 64, embedding_dim)\n",
    "        hidden                        => (batch_size, hidden_size = ??)\n",
    "        \n",
    "        hidden_with_time_axis => (batch_size, 1, hidden_size)\n",
    "        score                 => (batch_size, 64, hidden_size)\n",
    "        attention_weights     => (batch_size, 64, 1)\n",
    "        context_vector (after sum) => (batch_size, hidden_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam()\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    real => (batch_size = 8, (1))\n",
    "    pred => (batch_size = 8, vocab_size)\n",
    "    mask => Tensor(\"Cast:0\", shape=(batch_size,), dtype=float32)\n",
    "    loss_ => Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(batch_size,), dtype=float32)\n",
    "\n",
    "    return => Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "\n",
    "WORD_EMBEDDING = \"BERT\"\n",
    "RNN_TYPE = \"LSTM\"\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=32, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"add\",\n",
    "                 vocab_size=5000):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.combine_strategy = combine_strategy # [\"inject\", \"merge\"]\n",
    "        self.combine_layer = combine_layer # [\"inject\", \"merge\"]\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(vocab_size) # same size as vocab\n",
    "\n",
    "        # attention layer for image (CNN encoder output)\n",
    "        self.attention = BahdanauAttention(self.rnn_units)\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.bert_embedding.to('cuda')\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, x):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \n",
    "        x (Text tokens) => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            return self._bert_embedding(x)\n",
    "        else:\n",
    "            return self._default_embedding(x)\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x, output_layer=11):\n",
    "\n",
    "        # Format as torch Tensor\n",
    "        x = torch.as_tensor(x.numpy())\n",
    "        x = x.type(torch.LongTensor).to('cuda')\n",
    "        \n",
    "        # BERT's embedding\n",
    "        with torch.no_grad():\n",
    "            embedding , _ = self.bert_embedding(x)\n",
    "\n",
    "        # Revert back to tf.Tensor\n",
    "        x = embedding[output_layer].cpu().numpy()\n",
    "        x = tf.convert_to_tensor(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        return self.default_embedding(x)\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, h_state, c_state = self.lstm(x)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y, strategy=\"add\"):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, x, features, hidden):\n",
    "        \"\"\" \n",
    "        this using inject method,\n",
    "        image's feature injected at every timestep \n",
    "        \n",
    "        x : decoder input, also last word generated => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        context_vector = tf.expand_dims(context_vector, 1)\n",
    "\n",
    "        # x1 => (batch_size, 1, embedding_dim)\n",
    "        x1 = self.embedding(x)\n",
    "\n",
    "        \n",
    "        if self.combine_strategy == \"inject\":\n",
    "            \n",
    "            # x1 => (batch_size = 8, 1, combined_output_size)\n",
    "            x1 = self.custom_combine_layer(context_vector, x1)\n",
    "\n",
    "        # passing the concatenated vector to the RNN\n",
    "        output, state = self.rnn_model(x1)\n",
    "        \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "                \n",
    "        if self.combine_strategy == \"merge\":\n",
    "            \n",
    "            # output => (batchsize = 8, 1, combined_output_size)\n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "\n",
    "        # x3 shape => (batch_size, 1, rnn_units = 32)\n",
    "        x3 = self.fc1(output)\n",
    "\n",
    "        # x4 => (batch_size, rnn_units = 32)\n",
    "        x4 = tf.reshape(x3, (-1, x3.shape[2]))\n",
    "\n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        ## ====================================================\n",
    "        ## TODO: add PPLM framework here\n",
    "        ## feed-forward, find gradient, apply gradient, feed-forward again \n",
    "        ## ====================================================\n",
    "        \n",
    "        return word_predictions, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.rnn_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "\n",
    "STRATEGY = \"inject_par\" # [\"inject_par\", \"inject_pre\", \"inject_init\", \"merge\"]\n",
    "\n",
    "\n",
    "def train_step(img_tensor, target):\n",
    "    \n",
    "    loss = 0\n",
    "    batch_size = target.shape[0]\n",
    "    \n",
    "    ## Get image context vector\n",
    "    features = encoder(img_tensor)\n",
    "    \n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    if STRATEGY == \"inject_init\":\n",
    "        hidden = features\n",
    "    else:\n",
    "        hidden = decoder.reset_state(batch_size=batch_size)\n",
    "        \n",
    "\n",
    "    ## decoder_input == last word generated\n",
    "    if STRATEGY == \"inject_pre\":\n",
    "        decoder_input = features\n",
    "    else:\n",
    "        decoder_input = tf.expand_dims(target[:, 0], 1)\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            \n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "            \n",
    "            # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "            \n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 32\n",
    "IMAGE_FEATURE_DIM = 256\n",
    "WORD_EMBEDDING_DIM = 768 # BERT's embedding dim\n",
    "VOCAB_SIZE = len(tokenizer.vocab.keys()) # BERT's vocab size\n",
    "\n",
    "# WORD_EMBEDDING_DIM = 256\n",
    "# VOCAB_SIZE = 3000\n",
    "\n",
    "\n",
    "encoder = CNN_Encoder(\n",
    "    output_dim=IMAGE_FEATURE_DIM\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=\"LSTM\",\n",
    "    rnn_units=UNITS,\n",
    "    embedding_type=\"BERT\",\n",
    "    embedding_dim=WORD_EMBEDDING_DIM,  \n",
    "    combine_strategy=\"inject\", combine_layer=\"concat\",\n",
    "    vocab_size=VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]W0210 22:59:04.716404 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "1it [00:01,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.6576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 22:59:05.920240 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "2it [00:02,  1.61s/it]W0210 22:59:07.093510 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "3it [00:04,  1.48s/it]W0210 22:59:08.278572 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "4it [00:05,  1.39s/it]W0210 22:59:09.624444 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "5it [00:06,  1.38s/it]W0210 22:59:10.791342 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "6it [00:07,  1.32s/it]W0210 22:59:11.956666 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "7it [00:09,  1.27s/it]W0210 22:59:13.123088 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "8it [00:10,  1.24s/it]W0210 22:59:14.290271 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "9it [00:11,  1.22s/it]W0210 22:59:15.456096 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "10it [00:12,  1.20s/it]W0210 22:59:16.770183 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "11it [00:13,  1.24s/it]W0210 22:59:17.937262 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "12it [00:14,  1.22s/it]W0210 22:59:19.102513 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "13it [00:16,  1.20s/it]W0210 22:59:20.268681 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "14it [00:17,  1.19s/it]W0210 22:59:21.441089 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "15it [00:18,  1.18s/it]W0210 22:59:22.607433 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "16it [00:19,  1.18s/it]W0210 22:59:23.773171 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "17it [00:20,  1.18s/it]W0210 22:59:25.078518 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "18it [00:22,  1.21s/it]W0210 22:59:26.240485 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "19it [00:23,  1.20s/it]W0210 22:59:27.401839 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "20it [00:24,  1.19s/it]W0210 22:59:28.563669 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "21it [00:25,  1.18s/it]W0210 22:59:29.726041 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "22it [00:26,  1.17s/it]W0210 22:59:31.030802 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "23it [00:28,  1.21s/it]W0210 22:59:32.191848 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "24it [00:29,  1.20s/it]W0210 22:59:33.353374 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "25it [00:30,  1.19s/it]W0210 22:59:34.515000 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "26it [00:31,  1.21s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 98.789062\n",
      "Time taken for 1 epoch 31.619142532348633 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 22:59:36.078948 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "1it [00:01,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 3.1327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 22:59:37.248324 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "2it [00:02,  1.41s/it]W0210 22:59:38.559817 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "3it [00:03,  1.38s/it]W0210 22:59:39.721128 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "4it [00:05,  1.32s/it]W0210 22:59:40.884688 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "5it [00:06,  1.27s/it]W0210 22:59:42.046358 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "6it [00:07,  1.24s/it]W0210 22:59:43.209047 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "7it [00:08,  1.22s/it]W0210 22:59:44.375218 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "8it [00:09,  1.20s/it]W0210 22:59:45.685328 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "9it [00:11,  1.23s/it]W0210 22:59:46.849286 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "10it [00:12,  1.21s/it]W0210 22:59:48.014072 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "11it [00:13,  1.20s/it]W0210 22:59:49.177028 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "12it [00:14,  1.19s/it]W0210 22:59:50.342223 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "13it [00:15,  1.18s/it]W0210 22:59:51.506325 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "14it [00:16,  1.18s/it]W0210 22:59:52.815415 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "15it [00:18,  1.22s/it]W0210 22:59:53.980490 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "16it [00:19,  1.20s/it]W0210 22:59:55.147278 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "17it [00:20,  1.19s/it]W0210 22:59:56.313021 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "18it [00:21,  1.18s/it]W0210 22:59:57.477697 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "19it [00:22,  1.18s/it]W0210 22:59:58.670742 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "20it [00:24,  1.18s/it]W0210 22:59:59.986599 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "21it [00:25,  1.22s/it]W0210 23:00:01.148778 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "22it [00:26,  1.20s/it]W0210 23:00:02.313228 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "23it [00:27,  1.19s/it]W0210 23:00:03.476665 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "24it [00:28,  1.18s/it]W0210 23:00:04.639074 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "25it [00:30,  1.18s/it]W0210 23:00:05.803549 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "26it [00:31,  1.20s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 72.211014\n",
      "Time taken for 1 epoch 31.251070261001587 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:00:07.466242 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "1it [00:01,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 0 Loss 1.9787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:00:08.636275 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "2it [00:02,  1.51s/it]W0210 23:00:09.801339 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "3it [00:03,  1.41s/it]W0210 23:00:10.966423 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "4it [00:05,  1.33s/it]W0210 23:00:12.140205 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "5it [00:06,  1.29s/it]W0210 23:00:13.323706 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "6it [00:07,  1.25s/it]W0210 23:00:14.630395 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "7it [00:08,  1.27s/it]W0210 23:00:15.800873 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "8it [00:09,  1.24s/it]W0210 23:00:16.974657 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "9it [00:11,  1.22s/it]W0210 23:00:18.138060 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "10it [00:12,  1.20s/it]W0210 23:00:19.299620 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "11it [00:13,  1.19s/it]W0210 23:00:20.471090 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "12it [00:14,  1.19s/it]W0210 23:00:21.801201 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "13it [00:15,  1.23s/it]W0210 23:00:22.973855 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "14it [00:17,  1.21s/it]W0210 23:00:24.136094 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "15it [00:18,  1.20s/it]W0210 23:00:25.297498 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "16it [00:19,  1.19s/it]W0210 23:00:26.460328 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "17it [00:20,  1.18s/it]W0210 23:00:27.770380 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "18it [00:21,  1.22s/it]W0210 23:00:28.934230 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "19it [00:23,  1.20s/it]W0210 23:00:30.097971 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "20it [00:24,  1.19s/it]W0210 23:00:31.260862 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "21it [00:25,  1.18s/it]W0210 23:00:32.425056 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "22it [00:26,  1.18s/it]W0210 23:00:33.589244 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "23it [00:27,  1.17s/it]W0210 23:00:34.900937 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "24it [00:29,  1.21s/it]W0210 23:00:36.066429 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "25it [00:30,  1.20s/it]W0210 23:00:37.228570 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "26it [00:31,  1.21s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 53.423275\n",
      "Time taken for 1 epoch 31.42484164237976 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:00:38.737262 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "1it [00:01,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 0 Loss 1.8880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:00:39.920541 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "2it [00:02,  1.40s/it]W0210 23:00:41.086898 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "3it [00:03,  1.33s/it]W0210 23:00:42.401441 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "4it [00:05,  1.33s/it]W0210 23:00:43.569807 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "5it [00:06,  1.28s/it]W0210 23:00:44.736435 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "6it [00:07,  1.25s/it]W0210 23:00:45.901711 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "7it [00:08,  1.22s/it]W0210 23:00:47.066183 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "8it [00:09,  1.20s/it]W0210 23:00:48.230622 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "9it [00:10,  1.19s/it]W0210 23:00:49.540321 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "10it [00:12,  1.23s/it]W0210 23:00:50.705698 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "11it [00:13,  1.21s/it]W0210 23:00:51.868510 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "12it [00:14,  1.20s/it]W0210 23:00:53.031222 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "13it [00:15,  1.19s/it]W0210 23:00:54.198196 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "14it [00:16,  1.18s/it]W0210 23:00:55.367100 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "15it [00:18,  1.18s/it]W0210 23:00:56.682695 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "16it [00:19,  1.22s/it]W0210 23:00:57.862744 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "17it [00:20,  1.21s/it]W0210 23:00:59.036325 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "18it [00:21,  1.20s/it]W0210 23:01:00.212501 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "19it [00:22,  1.19s/it]W0210 23:01:01.386079 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "20it [00:24,  1.19s/it]W0210 23:01:02.561381 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "21it [00:25,  1.18s/it]W0210 23:01:03.732766 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "22it [00:26,  1.18s/it]W0210 23:01:05.069474 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "23it [00:27,  1.23s/it]W0210 23:01:06.254003 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "24it [00:29,  1.21s/it]W0210 23:01:07.436414 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "25it [00:30,  1.20s/it]W0210 23:01:08.608156 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "26it [00:31,  1.21s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 52.314682\n",
      "Time taken for 1 epoch 31.379693746566772 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:01:10.150508 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "1it [00:01,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 0 Loss 1.8880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:01:11.479221 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "2it [00:02,  1.47s/it]W0210 23:01:12.665232 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "3it [00:04,  1.39s/it]W0210 23:01:13.835566 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "4it [00:05,  1.32s/it]W0210 23:01:14.998605 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "5it [00:06,  1.27s/it]W0210 23:01:16.162382 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "6it [00:07,  1.24s/it]W0210 23:01:17.330033 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "7it [00:08,  1.22s/it]W0210 23:01:18.642434 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "8it [00:10,  1.25s/it]W0210 23:01:19.807908 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "9it [00:11,  1.22s/it]W0210 23:01:20.974285 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "10it [00:12,  1.21s/it]W0210 23:01:22.138054 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "11it [00:13,  1.19s/it]W0210 23:01:23.303387 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "12it [00:14,  1.18s/it]W0210 23:01:24.469866 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "13it [00:15,  1.18s/it]W0210 23:01:25.781866 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "14it [00:17,  1.22s/it]W0210 23:01:26.946944 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "15it [00:18,  1.20s/it]W0210 23:01:28.110553 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "16it [00:19,  1.19s/it]W0210 23:01:29.276138 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "17it [00:20,  1.18s/it]W0210 23:01:30.440639 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "18it [00:21,  1.18s/it]W0210 23:01:31.605338 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "19it [00:22,  1.17s/it]W0210 23:01:32.916382 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "20it [00:24,  1.22s/it]W0210 23:01:34.081908 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "21it [00:25,  1.20s/it]W0210 23:01:35.246026 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "22it [00:26,  1.19s/it]W0210 23:01:36.409341 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "23it [00:27,  1.18s/it]W0210 23:01:37.574327 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "24it [00:28,  1.18s/it]W0210 23:01:38.734548 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "25it [00:30,  1.17s/it]W0210 23:01:40.042311 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "26it [00:31,  1.21s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 52.103291\n",
      "Time taken for 1 epoch 31.433565378189087 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:01:41.552867 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "1it [00:01,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 0 Loss 1.8917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:01:42.716890 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "2it [00:02,  1.40s/it]W0210 23:01:43.881464 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "3it [00:03,  1.33s/it]W0210 23:01:45.049796 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "4it [00:04,  1.28s/it]W0210 23:01:46.213550 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "5it [00:06,  1.25s/it]W0210 23:01:47.523134 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "6it [00:07,  1.27s/it]W0210 23:01:48.690357 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "7it [00:08,  1.24s/it]W0210 23:01:49.856435 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "8it [00:09,  1.21s/it]W0210 23:01:51.022527 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "9it [00:10,  1.20s/it]W0210 23:01:52.186352 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "10it [00:12,  1.19s/it]W0210 23:01:53.353970 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "11it [00:13,  1.18s/it]W0210 23:01:54.667938 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "12it [00:14,  1.22s/it]W0210 23:01:55.833712 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "13it [00:15,  1.21s/it]W0210 23:01:56.999917 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "14it [00:16,  1.19s/it]W0210 23:01:58.166916 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "15it [00:18,  1.19s/it]W0210 23:01:59.334115 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "16it [00:19,  1.18s/it]W0210 23:02:00.499870 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "17it [00:20,  1.18s/it]W0210 23:02:01.812094 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "18it [00:21,  1.22s/it]W0210 23:02:02.988873 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "19it [00:22,  1.20s/it]W0210 23:02:04.157708 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "20it [00:24,  1.19s/it]W0210 23:02:05.320934 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "21it [00:25,  1.18s/it]W0210 23:02:06.486228 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "22it [00:26,  1.18s/it]W0210 23:02:07.648162 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "23it [00:27,  1.17s/it]W0210 23:02:08.958504 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "24it [00:28,  1.21s/it]W0210 23:02:10.121859 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "25it [00:30,  1.20s/it]W0210 23:02:11.284974 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "26it [00:31,  1.20s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 52.030350\n",
      "Time taken for 1 epoch 31.272177934646606 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:02:12.820463 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "1it [00:01,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 0 Loss 1.8934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:02:13.980929 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "2it [00:02,  1.40s/it]W0210 23:02:15.142256 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "3it [00:03,  1.33s/it]W0210 23:02:16.452113 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "4it [00:05,  1.32s/it]W0210 23:02:17.612950 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "5it [00:06,  1.27s/it]W0210 23:02:18.774574 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "6it [00:07,  1.24s/it]W0210 23:02:19.935915 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "7it [00:08,  1.22s/it]W0210 23:02:21.112108 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "8it [00:09,  1.20s/it]W0210 23:02:22.272789 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "9it [00:10,  1.19s/it]W0210 23:02:23.581382 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "10it [00:12,  1.23s/it]W0210 23:02:24.742855 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "11it [00:13,  1.21s/it]W0210 23:02:25.903822 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "12it [00:14,  1.19s/it]W0210 23:02:27.064635 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "13it [00:15,  1.18s/it]W0210 23:02:28.225693 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "14it [00:16,  1.18s/it]W0210 23:02:29.387058 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "15it [00:18,  1.17s/it]W0210 23:02:30.694532 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "16it [00:19,  1.21s/it]W0210 23:02:31.856686 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "17it [00:20,  1.20s/it]W0210 23:02:33.021306 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "18it [00:21,  1.19s/it]W0210 23:02:34.185111 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "19it [00:22,  1.18s/it]W0210 23:02:35.349555 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "20it [00:24,  1.18s/it]W0210 23:02:36.513704 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "21it [00:25,  1.17s/it]W0210 23:02:37.823267 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "22it [00:26,  1.21s/it]W0210 23:02:38.986950 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "23it [00:27,  1.20s/it]W0210 23:02:40.150350 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "24it [00:28,  1.19s/it]W0210 23:02:41.313489 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "25it [00:29,  1.18s/it]W0210 23:02:42.477880 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "26it [00:31,  1.20s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss 51.987473\n",
      "Time taken for 1 epoch 31.164012670516968 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:02:43.987537 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "1it [00:01,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 0 Loss 1.8944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0210 23:02:45.290378 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "2it [00:02,  1.44s/it]W0210 23:02:46.455057 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "3it [00:03,  1.36s/it]W0210 23:02:47.619931 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "4it [00:05,  1.30s/it]W0210 23:02:48.783938 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "5it [00:06,  1.26s/it]W0210 23:02:49.949466 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "6it [00:07,  1.23s/it]W0210 23:02:51.114118 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "7it [00:08,  1.21s/it]W0210 23:02:52.426226 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "8it [00:09,  1.24s/it]W0210 23:02:53.591243 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "9it [00:11,  1.22s/it]W0210 23:02:54.757758 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "10it [00:12,  1.20s/it]W0210 23:02:55.922387 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "11it [00:13,  1.19s/it]W0210 23:02:57.099420 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "12it [00:14,  1.19s/it]W0210 23:02:58.264328 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "13it [00:15,  1.18s/it]W0210 23:02:59.574085 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "14it [00:17,  1.22s/it]W0210 23:03:00.740751 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "15it [00:18,  1.20s/it]W0210 23:03:01.905884 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "16it [00:19,  1.19s/it]W0210 23:03:03.070266 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "17it [00:20,  1.18s/it]W0210 23:03:04.232626 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "18it [00:21,  1.18s/it]W0210 23:03:05.398846 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "19it [00:22,  1.17s/it]W0210 23:03:06.706538 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "20it [00:24,  1.21s/it]W0210 23:03:07.869946 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "21it [00:25,  1.20s/it]W0210 23:03:09.032188 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "22it [00:26,  1.19s/it]W0210 23:03:10.197991 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "23it [00:27,  1.18s/it]W0210 23:03:11.360453 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "24it [00:28,  1.18s/it]W0210 23:03:12.524982 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "25it [00:30,  1.17s/it]W0210 23:03:13.838929 139687631705920 optimizer_v2.py:979] Gradients does not exist for variables ['cnn__encoder_1/dense_6/kernel:0', 'cnn__encoder_1/dense_6/bias:0'] when minimizing the loss.\n",
      "26it [00:31,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 51.962730\n",
      "Time taken for 1 epoch 31.36051058769226 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset)):\n",
    "        \n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            \n",
    "        \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def eval_step(img_tensor, target):\n",
    "#     \"\"\"\n",
    "#     basically same as train_step, but doesn't apply gradient \n",
    "#     \"\"\"\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     # initializing the hidden state for each batch\n",
    "#     # because the captions are not related from image to image\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']] * target.shape[0], 1)\n",
    "    \n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         features = encoder(img_tensor)\n",
    "\n",
    "#         for i in range(1, target.shape[1]):\n",
    "            \n",
    "#             # passing the features through the decoder\n",
    "#             predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "\n",
    "#             loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "#             # using teacher forcing\n",
    "#             decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "#     total_loss = (loss / int(target.shape[1]))\n",
    "    \n",
    "#     return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Customized to enable multiple images, and incorporate PPLM framework\n",
    "\n",
    "\n",
    "# def custom_evaluate(image, supporting_images):\n",
    "    \n",
    "#     attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "#     # initializing the hidden state for decoder\n",
    "#     hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     # Extract image features\n",
    "#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "#     features = encoder(img_tensor_val)\n",
    "    \n",
    "    \n",
    "#     ## TODO: extract features from supporting images\n",
    "#     ## TODO: average those features into single context vector\n",
    "    \n",
    "    \n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     ## TODO: BERT doesn't have word_index\n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']], 0)\n",
    "#     result = []\n",
    "\n",
    "#     for i in range(max_length):\n",
    "#         \"\"\"\n",
    "#         Currently using inject method \n",
    "#         (image feature injected at every timestep)\n",
    "        \n",
    "#         TODO: test merge approach & pre, par inject\n",
    "#         \"\"\"\n",
    "        \n",
    "#         ## image feature inject's method\n",
    "#         predictions, hidden, attention_weights = decoder(decoder_input, features, hidden)\n",
    "        \n",
    "        \n",
    "#         ## TODO : apply PPLM here\n",
    "#         ## check loss (prediction - context vector of supporting images)\n",
    "#         ## apply gradient : hidden_state += diffrence(pred, supporting img vectors) (after n-iteration)\n",
    "#         ## re compute predictions\n",
    "        \n",
    "        \n",
    "#         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        \n",
    "        \n",
    "#         ## TODO: Allow using BERT tokenizer\n",
    "#         ## TODO: Revert predictions to index and words\n",
    "#         result.append(tokenizer.index_word[predicted_id])   # <<< return back id to text\n",
    "\n",
    "        \n",
    "#         if tokenizer.index_word[predicted_id] == '[END]':\n",
    "#             return result, attention_plot\n",
    "\n",
    "#         decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result), :]\n",
    "#     return result, attention_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
