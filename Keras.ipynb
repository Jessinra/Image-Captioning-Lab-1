{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 60\n",
    "\n",
    "\n",
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"START \" + annot['caption']\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "train_captions = all_captions[:NUM_SAMPLES]\n",
    "train_img_paths = all_img_paths[:NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 60\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"xception\"\n",
    "\n",
    "\n",
    "def get_encoder(model_type=MODEL_TYPE):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.keras.applications.xception\n",
    "        cnn_model = tf.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "encoder, encoder_preprocessor = get_encoder(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = encoder_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "# Get unique images\n",
    "unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# Prepare dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 8.5\n"
     ]
    }
   ],
   "source": [
    "estimated_batch_count = NUM_SAMPLES / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  5.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed image (batch)\n",
    "for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "    # get context vector of batch images\n",
    "    batch_features = encoder(batch_imgs)\n",
    "    \n",
    "    # flatten 2D cnn result into 1D for RNN decoder input\n",
    "    # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    # Cache preprocessed image\n",
    "    for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "        image_path = image_path.numpy().decode(\"utf-8\")\n",
    "        np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"BERT\"\n",
    "VOCAB_SIZE = 5000  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(x) for x in texts]\n",
    "        token_ids = [self.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        \n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOKENIZER == \"BERT\" :\n",
    "\n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "else : \n",
    "    \n",
    "    # use default keras tokenizer\n",
    "    tokenizer = TokenizerWrapper(num_words=VOCAB_SIZE, oov_token=\"[UNK]\")\n",
    "    tokenizer.fit_on_texts(train_captions)    \n",
    "    tokenizer.word_index['[PAD]'] = 0\n",
    "    tokenizer.index_word[0] = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "MAX_LENGTH = None  # use <int> or None\n",
    "\n",
    "\n",
    "train_captions = pad_sequences(train_captions, maxlen=MAX_LENGTH, padding='post', truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def load_image_npy(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "from tensorflow.data import Dataset\n",
    "dataset = Dataset.from_tensor_slices((train_img_paths, train_captions))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          load_image_npy, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(NUM_SAMPLES / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 8, 100, 2048)\n",
    "# caption => (batch_size = 8, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5 batches, (total : 40)\n",
      "eval : 1 batches, (total : 8)\n",
      "test : 2 batches, (total : 16 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam()\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    real => (batch_size = 8, (1))\n",
    "    pred => (batch_size = 8, vocab_size)\n",
    "    mask => Tensor(\"Cast:0\", shape=(batch_size,), dtype=float32)\n",
    "    loss_ => Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(batch_size,), dtype=float32)\n",
    "\n",
    "    return => Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, 64, embedding_dim)\n",
    "        hidden                        => (batch_size, hidden_size = ??)\n",
    "        \n",
    "        hidden_with_time_axis => (batch_size, 1, hidden_size)\n",
    "        score                 => (batch_size, 64, hidden_size)\n",
    "        attention_weights     => (batch_size, 64, 1)\n",
    "        context_vector (after sum) => (batch_size, hidden_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, 64, output_dim)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "\n",
    "WORD_EMBEDDING = \"BERT\"\n",
    "RNN_TYPE = \"LSTM\"\n",
    "STRATEGY = \"inject\"\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=32, embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 strategy=\"merge\", vocab_size=5000):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.strategy = strategy\n",
    "        \n",
    "\n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        else:\n",
    "            self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.rnn = LSTM(self.rnn_units,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.rnn = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "           \n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(vocab_size) # same size as vocab\n",
    "\n",
    "        # attention layer for image (CNN encoder output)\n",
    "        self.attention = BahdanauAttention(self.rnn_units)\n",
    "        \n",
    "        \n",
    "    def get_embedding(self, x):\n",
    "        if self.embedding_type == \"BERT\": \n",
    "            return self._get_bert_embedding(x)\n",
    "        else:\n",
    "            return self._get_default_embedding(x)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _get_bert_embedding(self, x, output_layer=11):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \n",
    "        x (Text tokens) => (batch_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Format as torch Tensor\n",
    "        x = torch.as_tensor(x.numpy())\n",
    "        x = x.type(torch.LongTensor)\n",
    "        \n",
    "        # BERT's embedding\n",
    "        with torch.no_grad():\n",
    "            embedding , _ = self.bert_model(x)\n",
    "\n",
    "        # Revert back to tf.Tensor\n",
    "        x = embedding[output_layer].numpy()\n",
    "        x = tf.convert_to_tensor(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def _get_default_embedding(self, x):\n",
    "        \"\"\"\n",
    "        Get default embedding for text tokens\n",
    "        \n",
    "        x (Text tokens) => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "        \n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        \"\"\" \n",
    "        this using inject method,\n",
    "        image's feature injected at every timestep \n",
    "        \n",
    "        x : decoder input, also last word generated => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x1 => (batch_size, 1, embedding_dim)\n",
    "        x1 = self.get_embedding(x)\n",
    "\n",
    "        \n",
    "        ## TODO: inject image here ?\n",
    "        ## TODO: add PPLM framework here\n",
    "        ## feed-forward, find gradient, apply gradient, feed-forward again \n",
    "        \n",
    "\n",
    "        ## Concat context vector + hidden state => inject method\n",
    "        ## TODO : use merge method, pre method\n",
    "        \n",
    "        if self.strategy == \"inject\":\n",
    "            # x2 => (batch_size, 1, image_feature_size + embedding_dim)\n",
    "            x2 = tf.concat([tf.expand_dims(context_vector, 1), x1], axis=-1)\n",
    "        else:\n",
    "            x2 = x1\n",
    "            \n",
    "            \n",
    "        # passing the concatenated vector to the RNN (LSTM / GRU)\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, h_state, c_state = self.rnn(x2)\n",
    "            state = h_state\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, state = self.rnn(x2)\n",
    "        \n",
    "\n",
    "        ## ======================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ======================\n",
    "                \n",
    "        if self.strategy == \"merge\":\n",
    "            \n",
    "            # output => (batchsize = 8, 1, image_feature_size + embedding_dim)\n",
    "            output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
    "            \n",
    "            # if using add instead of concat\n",
    "            # output = tf.keras.layers.add([context_vector, output])\n",
    "\n",
    "            \n",
    "        # x3 shape => (batch_size, 1, rnn_units = 32)\n",
    "        x3 = self.fc1(output)\n",
    "\n",
    "        # x4 => (batch_size, rnn_units = 32)\n",
    "        x4 = tf.reshape(x3, (-1, x3.shape[2]))\n",
    "\n",
    "        \n",
    "        ## TODO : merge method, concat context vector + hiddens state here .\n",
    "        \n",
    "        \n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "\n",
    "        \n",
    "        return word_predictions, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.rnn_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "\n",
    "def train_step(img_tensor, target):\n",
    "    \n",
    "    loss = 0\n",
    "    batch_size = target.shape[0]\n",
    "    \n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "    ## decoder_input == last word generated\n",
    "    decoder_input = tf.expand_dims(target[:, 0], 1)\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        ## Get image context vector\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            \n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "            \n",
    "            # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "            \n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 32\n",
    "IMAGE_FEATURE_DIM = 256\n",
    "# WORD_EMBEDDING_DIM = 768 # BERT's embedding dim\n",
    "# VOCAB_SIZE = len(tokenizer.vocab.keys()) # BERT's vocab size\n",
    "\n",
    "WORD_EMBEDDING_DIM = 256\n",
    "VOCAB_SIZE = 3000\n",
    "\n",
    "\n",
    "encoder = CNN_Encoder(\n",
    "    output_dim=IMAGE_FEATURE_DIM\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=\"LSTM\",\n",
    "    rnn_units=UNITS,\n",
    "    embedding_type=\"BERT\",\n",
    "    embedding_dim=WORD_EMBEDDING_DIM,  \n",
    "    vocab_size=VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n",
      "word_predictions (8, 3000)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=222241, shape=(), dtype=float32, numpy=24.316196>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss = 0\n",
    "\n",
    "for img_tensor, target in train_dataset:\n",
    "    batch_loss, t_loss = train_step(img_tensor, target)\n",
    "    total_loss += t_loss\n",
    "    \n",
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset)):\n",
    "        \n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            \n",
    "        \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def eval_step(img_tensor, target):\n",
    "#     \"\"\"\n",
    "#     basically same as train_step, but doesn't apply gradient \n",
    "#     \"\"\"\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     # initializing the hidden state for each batch\n",
    "#     # because the captions are not related from image to image\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']] * target.shape[0], 1)\n",
    "    \n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         features = encoder(img_tensor)\n",
    "\n",
    "#         for i in range(1, target.shape[1]):\n",
    "            \n",
    "#             # passing the features through the decoder\n",
    "#             predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "\n",
    "#             loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "#             # using teacher forcing\n",
    "#             decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "#     total_loss = (loss / int(target.shape[1]))\n",
    "    \n",
    "#     return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Customized to enable multiple images, and incorporate PPLM framework\n",
    "\n",
    "\n",
    "# def custom_evaluate(image, supporting_images):\n",
    "    \n",
    "#     attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "#     # initializing the hidden state for decoder\n",
    "#     hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     # Extract image features\n",
    "#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "#     features = encoder(img_tensor_val)\n",
    "    \n",
    "    \n",
    "#     ## TODO: extract features from supporting images\n",
    "#     ## TODO: average those features into single context vector\n",
    "    \n",
    "    \n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     ## TODO: BERT doesn't have word_index\n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']], 0)\n",
    "#     result = []\n",
    "\n",
    "#     for i in range(max_length):\n",
    "#         \"\"\"\n",
    "#         Currently using inject method \n",
    "#         (image feature injected at every timestep)\n",
    "        \n",
    "#         TODO: test merge approach & pre, par inject\n",
    "#         \"\"\"\n",
    "        \n",
    "#         ## image feature inject's method\n",
    "#         predictions, hidden, attention_weights = decoder(decoder_input, features, hidden)\n",
    "        \n",
    "        \n",
    "#         ## TODO : apply PPLM here\n",
    "#         ## check loss (prediction - context vector of supporting images)\n",
    "#         ## apply gradient : hidden_state += diffrence(pred, supporting img vectors) (after n-iteration)\n",
    "#         ## re compute predictions\n",
    "        \n",
    "        \n",
    "#         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        \n",
    "        \n",
    "#         ## TODO: Allow using BERT tokenizer\n",
    "#         ## TODO: Revert predictions to index and words\n",
    "#         result.append(tokenizer.index_word[predicted_id])   # <<< return back id to text\n",
    "\n",
    "        \n",
    "#         if tokenizer.index_word[predicted_id] == '[END]':\n",
    "#             return result, attention_plot\n",
    "\n",
    "#         decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result), :]\n",
    "#     return result, attention_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
