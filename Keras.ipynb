{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 150\n",
    "\n",
    "\n",
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"START \" + annot['caption']\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "train_captions = all_captions[:NUM_SAMPLES]\n",
    "train_img_paths = all_img_paths[:NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 150\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(model_type=\"xception\"):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.keras.applications.xception\n",
    "        cnn_model = tf.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"CNN encoder model not supported yet\")\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"xception\"\n",
    "\n",
    "\n",
    "encoder, encoder_preprocessor = get_encoder(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = encoder_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "# Get unique images\n",
    "unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# Prepare dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 19.75\n"
     ]
    }
   ],
   "source": [
    "estimated_batch_count = NUM_SAMPLES / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:04,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed image (batch)\n",
    "for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "    # get context vector of batch images\n",
    "    batch_features = encoder(batch_imgs)\n",
    "    \n",
    "    # flatten 2D cnn result into 1D for RNN decoder input\n",
    "    # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    # Cache preprocessed image\n",
    "    for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "        image_path = image_path.numpy().decode(\"utf-8\")\n",
    "        np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"BERT\"\n",
    "VOCAB_SIZE = 5000  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(x) for x in texts]\n",
    "        token_ids = [self.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        \n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_type=\"BERT\"):\n",
    "    \n",
    "    if tokenizer_type == \"BERT\" :\n",
    "\n",
    "        # Load pre-trained BERT tokenizer (vocabulary)\n",
    "        tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    else : \n",
    "\n",
    "        # use default keras tokenizer\n",
    "        tokenizer = TokenizerWrapper(num_words=VOCAB_SIZE, oov_token=\"[UNK]\")\n",
    "        tokenizer.fit_on_texts(train_captions)    \n",
    "        tokenizer.word_index['[PAD]'] = 0\n",
    "        tokenizer.index_word[0] = '[PAD]'\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(tokenizer_type=\"BERT\")\n",
    "train_captions = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = None  # use <int> or None\n",
    "\n",
    "\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "train_captions = pad_sequences(train_captions, maxlen=MAX_LENGTH, padding='post', truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "\n",
    "def load_image_npy(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "dataset = Dataset.from_tensor_slices((train_img_paths, train_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          load_image_npy, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "\n",
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(NUM_SAMPLES / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 13 batches, (total : 104)\n",
      "eval : 2 batches, (total : 16)\n",
      "test : 4 batches, (total : 32 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, 64, output_dim)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, 64, embedding_dim)\n",
    "        hidden                        => (batch_size, hidden_size = ??)\n",
    "        \n",
    "        hidden_with_time_axis => (batch_size, 1, hidden_size)\n",
    "        score                 => (batch_size, 64, hidden_size)\n",
    "        attention_weights     => (batch_size, 64, 1)\n",
    "        context_vector (after sum) => (batch_size, hidden_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam()\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    real => (batch_size = 16, (1))\n",
    "    pred => (batch_size = 16, vocab_size)\n",
    "    mask => Tensor(\"Cast:0\", shape=(batch_size,), dtype=float32)\n",
    "    loss_ => Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(batch_size,), dtype=float32)\n",
    "\n",
    "    return => Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=32, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=5000):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(self.vocab_size) # same size as vocab\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            \n",
    "            self.bert_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.bert_embedding.to('cuda')\n",
    "            \n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            self.vocab_size = self.bert_embedding.config.vocab_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, x):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \n",
    "        x (Text tokens) => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            return self._bert_embedding(x)\n",
    "        else:\n",
    "            return self._default_embedding(x)\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x, output_layer=11):\n",
    "\n",
    "        # Format as torch Tensor\n",
    "        x = torch.as_tensor(x.numpy())\n",
    "        x = x.type(torch.LongTensor).to('cuda')\n",
    "        \n",
    "        # BERT's embedding\n",
    "        with torch.no_grad():\n",
    "            embedding , _ = self.bert_embedding(x)\n",
    "\n",
    "        # Revert back to tf.Tensor\n",
    "        x = embedding[output_layer].cpu().numpy()\n",
    "        x = tf.convert_to_tensor(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        return self.default_embedding(x)\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=0):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.combine_strategy == \"inject_init\":\n",
    "            initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "            output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "        elif self.combine_strategy == \"inject_pre\":\n",
    "            x = context_vector if curr_iter == 1 else x\n",
    "            output, state = self.rnn_model(x)  \n",
    "            \n",
    "        elif self.combine_strategy == \"inject_par\":\n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            output, state = self.rnn_model(x)              \n",
    "\n",
    "        else: # merge (as default)\n",
    "            output, state = self.rnn_model(x)           \n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, decoder_input, context_vector, iteration):\n",
    "        \"\"\" \n",
    "        decoder_input : last predicted word => (batch_size, 1)\n",
    "        context_vector : image's vector\n",
    "        \"\"\"\n",
    "\n",
    "        # x1 => (batch_size, 1, embedding_dim)\n",
    "        x1 = self.embedding(decoder_input)\n",
    "        \n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector, iteration)\n",
    "            \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "            \n",
    "        # x3 shape => (batch_size, 1, rnn_units = 32)\n",
    "        x3 = self.fc1(x2)\n",
    "\n",
    "        # x4 => (batch_size, rnn_units = 32)\n",
    "        x4 = tf.reshape(x3, (-1, x3.shape[2]))\n",
    "\n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        ## ====================================================\n",
    "        ## TODO: add PPLM framework here\n",
    "        ## feed-forward, find gradient, apply gradient, feed-forward again \n",
    "        ## ====================================================\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.rnn_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "\n",
    "\n",
    "def train_step(img_tensor, target):\n",
    "    \n",
    "    loss = 0\n",
    "    batch_size = target.shape[0]\n",
    "    \n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    ## decoder_input == last word generated\n",
    "    decoder_input = tf.expand_dims(target[:, 0], 1)\n",
    "\n",
    "    # Training model\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        ## Get image context vector\n",
    "        features = encoder(img_tensor)\n",
    "        \n",
    "        for i in range(1, target.shape[1]):\n",
    "\n",
    "            # Getting image feature / context_vector from encoder -> attention model\n",
    "            context_vector, attention_weights = attention(features, hidden)\n",
    "            context_vector = tf.expand_dims(context_vector, 1)     \n",
    "            \n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden = decoder(decoder_input, context_vector, iteration=i)\n",
    "            \n",
    "            # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "            \n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables + attention.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 32\n",
    "RNN_TYPE = \"LSTM\"\n",
    "IMAGE_FEATURE_DIM = 256  # default : 256\n",
    "WORD_EMBEDDING = \"BERT\"\n",
    "WORD_EMBEDDING_DIM = 256\n",
    "VOCAB_SIZE = 3000\n",
    "COMBINE_STRATEGY = \"inject_par\"\n",
    "COMBINE_LAYER = \"concat\"\n",
    "\n",
    "\n",
    "encoder = CNN_Encoder(output_dim=IMAGE_FEATURE_DIM)\n",
    "\n",
    "attention = BahdanauAttention(units=UNITS)\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=\"LSTM\", rnn_units=UNITS,\n",
    "    embedding_type=\"BERT\", embedding_dim=WORD_EMBEDDING_DIM,  \n",
    "    combine_strategy=\"inject_par\", \n",
    "    combine_layer=\"concat\",\n",
    "    vocab_size=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_FEATURE_DIM == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_FEATURE_DIM == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset)):\n",
    "\n",
    "    batch_loss, t_loss = train_step(img_tensor, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:12,  1.00it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 58.135864\n",
      "Time taken for 1 epoch 13.007756233215332 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 4.0924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:12,  1.02it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 49.251560\n",
      "Time taken for 1 epoch 12.721926927566528 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 0 Loss 3.2757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:12,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 38.586624\n",
      "Time taken for 1 epoch 12.865242719650269 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset)):\n",
    "        \n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            \n",
    "        \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XeYFFXWx/HvmUDOMCAwCBIUwUAYkBxMRBEzRlRcxEBQUVd3fXVdXV0VBRQDYk4YUSQpSs4MOUsWECQnyXDeP6bYncUZaGC6e8Lv8zz9TPWtul1nioZD1a26x9wdERGRE4mJdgAiIpI1KGGIiEhIlDBERCQkShgiIhISJQwREQmJEoaIiIRECUMkkzGzZma2NtpxiBxLCUNyNDNbZWaXRmG/t5vZYTPbbWY7zWy2mbU9hc9538yeCUeMIsdSwhCJnsnuXgAoArwDfGFmRaMck0i6lDBE0mFmfzGzZWa21cwGm1mZoN3M7BUz2xicHcwzs/OCda3NbKGZ7TKzdWbW80T7cfcjwLtAXqBSGnGca2ZjzGy7mS0ws3ZBe2fgZuCR4Ezl+wz89UX+RAlDJA1mdjHwHHA9UBpYDQwMVl8ONAHOBgoH22wJ1r0D3O3uBYHzgFEh7CsOuAvYDSw9Zl088D3wI1AS6Ap8YmbnuHt/4BPgBXcv4O5XnPIvLBICJQyRtN0MvOvuM919P/AYUN/MKgAHgYJAVcDcfZG7rw/6HQSqmVkhd9/m7jOPs496ZrYd2ADcCFzl7juO3QYoADzv7gfcfRQwJNheJKKUMETSVoaUswoA3H03KWcRZYN/tF8D+gEbzay/mRUKNr0GaA2sNrOxZlb/OPuY4u5F3L2Eu9dz95/SiWNNcNnqqNVA2VP/1UROjRKGSNp+A8offWNm+YHiwDoAd+/r7rWBaqRcmno4aJ/u7leScvnoW+CLDIijnJml/rt65tE4AE03LRGjhCEC8WaWJ9UrDvgMuMPMaphZbuBfwFR3X2VmdczsomB84Q9gH3DEzHKZ2c1mVtjdDwI7gSPp7jU0U4E9pAxsx5tZM+AK/jue8jtQ8TT3IRISJQwRGAbsTfV6Krg89ATwNbCelLuXOgTbFwLeBraRcnloC/BisO5WYJWZ7QS6kDIWcsrc/QApCaIVsBl4HbjN3RcHm7xDypjJdjP79nT2JXIipgJKIiISCp1hiIhISJQwREQkJEoYIiISEiUMEREJSVy0A8hIJUqU8AoVKkQ7DBGRLGPGjBmb3T0hlG2zVcKoUKECycnJ0Q5DRCTLMLPVJ94qhS5JiYhISJQwREQkJEoYIiISEiUMEREJiRKGiIiERAlDRERCooQhIiIhUcIA+v68lJm/bot2GCIimVqOTxg79hzk06m/cvXrk3joizls3LUv2iGJiGRKOT5hFM4Xz08PNaVL00oMnrOOS14ay4DxKzh4+HQLpYmIZC85PmEAFMgdx19bVeWHHk2oVb4ozwxdROs+45m4bHO0QxMRyTSUMFKpmFCA9++ow9u3JbH/0BFuHjCVez6ewdpte6IdmohI1IV18kEzWwXsAg4Dh9w9ycw+B84JNikCbHf3GqH0DWesqfbLZdVK0bhKCd4et4J+Y5YxeslG7mlambubViRPfGwkwhARyXTCWtM7+Ec/yd3TvLZjZr2AHe7+9Mn2TUtSUpJn9Gy167bv5V9DFzF03nrKFcvLE22qcVm1UphZhu5HRCQazGxGqP8hj9olKUv5F/d64LNoxRCKskXy0u/mWnx610XkjY+l80cz6PjedJZv2h3t0EREIircCcOBH81shpl1PmZdY+B3d196Cn3/w8w6m1mymSVv2rQpg8L+swaVSzC0W2P+r201Zq3eRsve43hu2CJ27z8Utn2KiGQm4b4kVdbd15lZSWAk0NXdxwXr3gCWuXuvk+2bnnBckkrLpl37eWHEYr6csZaSBXPzWOuqtK9RVpepRCTLyTSXpNx9XfBzIzAIqAtgZnHA1cDnJ9s3M0gomJsXr7uQQfc2oHThPDzw+Ryue3My89ftiHZoIiJhE7aEYWb5zazg0WXgcmB+sPpSYLG7rz2FvplGzTOLMujehvz7mvNZufkP2r02gb9/O49tfxyIdmgiIhkunGcYpYAJZjYHmAYMdfcRwboOHDPYbWZlzGxYCH0zlZgY44Y6ZzKqZzNuq1+Bz6atoXmvMXw8ZTWHj4Tvcp+ISKSFdQwj0iI1hnE8izfs5MnvFjB15VaqlynEP9pVJ6lCsajGJCKSnkwzhpETVT2jEAM71+PVG2uy9Y8DXPvmZB74fDYbd2pSQxHJ2pQwwsDMuOLCMvz8UFPua16JoXPX0/ylMbw1djkHDmlSQxHJmpQwwihfrjgeblGVHx9oQr2KxXlu+GJa9hnH2F/C97yIiEi4KGFEQIUS+Xnn9jq8d3sdjhxxOr47jc4fJrNmqyY1FJGsQwkjgppXLckPDzThkZbnMH7pZi59eSwvj/yFvQcORzs0EZETUsKIsNxxsdzbrDKjejbl8upn0PfnpVz68liGz1tPdrpjTUSyHyWMKCldOC+v3liTgZ3rUTBPHPd8MpNb3pnK0t93RTs0EZE0KWFEWb2KxRnStRH/aFedeWt30KrPeJ4ZspBd+w5GOzQRkf+hhJEJxMXG0LFBBUb3bMa1tRN5Z+JKmr80lq9mrOWInhYXkUxCCSMTKV4gN89fcwHf3deQxKJ56fnlHK55cxLz1mpSQxGJPiWMTOiCxCJ8c08DXrz2AtZs3UO7fhN47Ju5bNWkhiISRUoYmVRMjHFdUjlG9WzGnQ3P4ovktTR7cTQfTl7FocN6WlxEIk8JI5MrlCeeJ9pWY0T3xpyfWJj/+24BbV+dwNQVW6IdmojkMEoYWUSVUgX5uNNFvHFzLXbtO8QN/afQ7bNZbNihSQ1FJDKUMLIQM6PV+aX56cGmdLu4MiMWbODiXmN4fcwy9h/S0+IiEl5KGFlQ3lyxPHj5Ofz0QFMaVi7BCyOW0LL3eEYv3hjt0EQkGwtrwjCzVWY2z8xmm1ly0PaUma0L2mabWet0+rY0syVmtszM/hrOOLOqM4vn4+3bkvjgzroYcMf70+n0/nRWb/kj2qGJSDYU1op7ZrYKSHL3zanangJ2u/tLx+kXC/wCXAasBaYDN7r7wuPtLzNU3IuWA4eO8N7ElfT9eSkHDzudm1Tk3uaVyJcrLtqhiUgmlh0q7tUFlrn7Cnc/AAwEroxyTJlarrgY7m5aiVE9m9HmgtK8NnoZl/Yay5C5v2lSQxHJEOFOGA78aGYzzKxzqvb7zWyumb1rZkXT6FcWWJPq/dqg7U/MrLOZJZtZ8qZNKkxUqlAeXrmhBl92qU+RfLm4/9NZ3PT2VJZs0KSGInJ6wp0wGrl7LaAVcJ+ZNQHeACoBNYD1QK/T2YG793f3JHdPSkhIOO2As4s6FYrxfddG/LP9eSzasJPWfcfzj+8XsGOvJjUUkVMT1oTh7uuCnxuBQUBdd//d3Q+7+xHgbVIuPx1rHVAu1fvEoE1OQmyMcWu98ox+qBkd6pTj/UmruPilMXwxfY0mNRSRkxa2hGFm+c2s4NFl4HJgvpmVTrXZVcD8NLpPB6qY2VlmlgvoAAwOV6zZXdH8uXj2qvP5/v5GnFUiP498PZer3pjE7DXbox2aiGQh4TzDKAVMMLM5wDRgqLuPAF4IbrWdCzQHHgAwszJmNgzA3Q8B9wM/AIuAL9x9QRhjzRHOK1uYL7vU55UbLuS37Xtp328ij3w1h82790c7NBHJAsJ6W22k5eTbak/Wrn0HeXXUMt6dsJK8uWJ54NKzua1+eeJiM+uNcyISDtnhtloJs4J54nm89bmM6NGEGuWK8PSQhbTuO55JyzefuLOI5EhKGDlc5ZIF+PDOurx1a232HDjMTW9P5b5PZ/Lb9r3RDk1EMhklDMHMaFH9DH56sCkPXHo2Py38nUt6jeW1UUvZd1CTGopICiUM+Y888bF0v7QKPz/UlGbnJPDSj79w+Svj+Gnh73paXESUMOTPEovm441bavNxp4vIFRfDXR8mc8f701m5WZMaiuRkShiSrkZVSjC8e2P+3uZckldto8Ur4/j3iMX8sf9QtEMTkShQwpDjio+N4a7GFRnVsylXXFiGN8Ys55JeY/lu9jpdphLJYZQwJCQlC+ah1/UX8vU9DUgomJvuA2dzQ/8pLFq/M9qhiUiEKGHISaldvijf3teQ564+n6W/76JN3/H833fz2b7nQLRDE5EwU8KQkxYbY9xY90zG9GzOrfXK8/GU1TR/aQyfTv2Vw5rUUCTbUsKQU1Y4Xzz/uPI8hnRtTJWSBXl80Dza95vIjNXboh2aiISBEoactmplCvH53fXo06EGG3ft45o3JvHQF3PYuGtftEMTkQykhCEZwsy4skZZRj3UjHuaVWLwnHVc/NJYBoxfwcHDR6IdnohkACUMyVD5c8fxaMuq/PhAU5IqFOWZoYto1Wc8E5ZqUkORrE4JQ8LirBL5ee/2Ogy4LYkDh45wyztT6fLRDNZu2xPt0ETkFClhSNiYGZdWK8WPDzSh5+VnM/aXTVzSayy9f/pFkxqKZEFhLaBkZquAXcBh4JC7J5nZi8AVwAFgOXCHu/+pVmhafU+0PxVQytx+276XZ4ctYujc9SQWzcsTbatxebVSmFm0QxPJsTJbAaXm7l4jVUAjgfPc/QLgF+Cxk+grWViZInnpd1MtPv3LReTPFcfdH83gtnensWzj7miHJiIhiPglKXf/MajZDTAFSIx0DBJdDSqVYGi3Rjx5RTVmr9lOy97j+NewRezadzDaoYnIcYQ7YTjwo5nNMLPOaay/Exh+in0BMLPOZpZsZsmbNm3KgJAlEuJiY7ij4VmM7tmMa2ol0n/cCi7uNZZBs9ZqUkORTCrcYxhl3X2dmZUk5VJUV3cfF6z7G5AEXO1pBHG8vunRGEbWNXvNdp78bj5z1u4gqXxRnmpXnfPKFo52WCLZXqYZw3D3dcHPjcAgoC6Amd0OtAVuTitZHK+vZE81yhVh0L0NeeGaC1i5+Q+ueG0Cfxs0j21/aFJDkcwibAnDzPKbWcGjy8DlwHwzawk8ArRz9zRvyk+vb7hilcwhJsa4vk45RvVsxu0NKjBw+hqa9xrDR1NWa1JDkUwgnGcYpYAJZjYHmAYMdfcRwGtAQWCkmc02szcBzKyMmQ07QV/JAQrnjefJK6ozrFtjqp5RkCe+nc8Vr05g+qqt0Q5NJEcL6xhGpGkMI/txd4bOW8+zQxexfsc+2tcow2Otz6VUoTzRDk0kW8g0Yxgip8vMaHtBGX5+qCn3N6/MsHkbuPilMbw1djkHDmlSQ5FIUsKQLCFfrjh6tjiHkQ82oX6l4jw3fDEt+4xj7C+6lVokUpQwJEspXzw/AzrW4b076uAOHd+dxl8+TObXLZrUUCTclDAkS2p+TklG9GjMoy2rMnHZZi59ZSwv/7iEvQc0qaFIuChhSJaVOy6We5pVYtRDzWhZ/Qz6jlrGpS+PZdi89XpaXCQMlDAkyzujcB763liTzzvXo2CeOO79ZCa3vDOVpb/vinZoItmKEoZkGxdVLM6Qro14+srqzFu7g1Z9xvPPIQvZqUkNRTKEEoZkK3GxMdxWvwJjHm7OdUnleHfiSi5+aSxfJq/hiJ4WFzktShiSLRXLn4vnrj6fwfc14sxieXn4q7lc8+Yk5q79U60uEQmREoZka+cnFuarLg3odd2FrNm6lyv7TeSxb+ayZff+aIcmkuUoYUi2FxNjXFM7kVE9m9Kp4Vl8mbyW5i+N4YNJqzh0WE+Li4RKCUNyjEJ54vl722qM6NGYCxKL8OTgBbR9dQJTV2yJdmgiWYIShuQ4lUsW5KNOdXnzllrs2neIG/pPoetns1i/Y2+0QxPJ1JQwJEcyM1qeV5qfHmxKt0uq8MOCDVzSayz9Ri9j/yE9LS6SFiUMydHy5orlwcvO5ucHm9Kocgle/GEJLV4Zx+jFG6Mdmkimo4QhApQrlo/+tyXx4Z11iYkx7nh/Op3en86qzX9EOzSRTCOsCcPMVpnZvKCyXnLQVszMRprZ0uBn0XT6dgy2WWpmHcMZp8hRTc5OYET3JjzeuipTVmzh8lfG8eIPi9lz4FC0QxOJurBW3DOzVUCSu29O1fYCsNXdnzezvwJF3f3RY/oVA5KBJMCBGUBtd992vP2p4p5kpI079/H88MV8M2sdpQvn4fHW59L2gtKYWbRDE8kwmb3i3pXAB8HyB0D7NLZpAYx0961BkhgJtIxQfCIAlCyUh5dvqMFXXepTNF8uun42ixvfnsLiDTujHZpIVIQ7YTjwo5nNMLPOQVspd18fLG8ASqXRryywJtX7tUGbSMQlVSjG910b8Uz781i8YRdt+k7gqcEL2LFXkxpKzhIX5s9v5O7rzKwkMNLMFqde6e5uZqd1TSxIRJ0BzjzzzNP5KJF0xcYYt9QrT5vzS9Nr5BI+nLyK7+f8xiMtz+G62uWIidFlKsn+wnqG4e7rgp8bgUFAXeB3MysNEPxM6/7FdUC5VO8Tg7a09tHf3ZPcPSkhISEjwxf5k6L5c/FM+/P5vmsjKibk59Gv53HV6xOZvUaTGkr2F7aEYWb5zazg0WXgcmA+MBg4etdTR+C7NLr/AFxuZkWDu6guD9pEMoXqZQrzxd316X1DDdbv2Ef7fhN5+Ms5bNqlSQ0l+wrnJalSwKDgjpI44FN3H2Fm04EvzKwTsBq4HsDMkoAu7n6Xu281s38C04PPetrdt4YxVpGTZma0r1mWS6uV4tVRS3l3wkpGzN/AA5edza31yxMfq8ecJHsJ6221kabbaiWalm/azdPfL2TsL5s4u1QBnmpXnQaVSkQ7LJHjyuy31YpkS5USCvD+HXV4+7Yk9h48zE1vT+W+T2aybrsmNZTsQQlDJAOZGZdVK8XIB5qmzFG1+Hcu6TWGN8cu57BKxEoWp4QhEgZ54mPpdkkVfnqwKU3PTuD54Yu56e0pOtuQLC2khGFmlcwsd7DczMy6mVmR8IYmkvUlFs3Hm7fU5qXrLmT+uh207D2OwXN+i3ZYIqck1DOMr4HDZlYZ6E/KMxKfhi0qkWzEzLi2diLDuzehSskCdPtsFg9+Pptd+/SkuGQtoSaMI+5+CLgKeNXdHwZKhy8skeznzOL5+OLu+vS4tArfzfmNVn3Gk7xKd4tL1hFqwjhoZjeS8qDdkKAtPjwhiWRfcbEx9Lj0bL64uz4xZlz/1mR6/biEg4ePRDs0kRMKNWHcAdQHnnX3lWZ2FvBR+MISyd5qly/KsO6NubpWIq+OWsa1b05WsSbJ9E76wb1gqo5y7j43PCGdOj24J1nR0LnreXzQPA4ePsJTV1TnuqRE1dyQiMnwB/fMbIyZFQoKG80E3jazl08nSBFJ0eaC0ozo0ZgLE4vwyNdzuefjmWz740C0wxL5k1AvSRV2953A1cCH7n4RcGn4whLJWUoXzssnd13EY62q8vPi32nZZxwTlm4+cUeRCAo1YcQFU5Ffz38HvUUkA8XEGHc3rcSgextSME88t7wzlWeHLmT/ocPRDk0ECD1hPE3K9OLL3X26mVUEloYvLJGc67yyhfn+/kbcWq88b49fSft+k/jl913RDktEs9WKZGY/L/qdR76ay+79h3i89bncVr+8BsQlQ4Vj0DvRzAaZ2cbg9bWZJZ5emCJyIpecW4oRPZrQoFJxnhy8gDven87GXfuiHZbkUKFeknqPlEp5ZYLX90GbiIRZQsHcvHt7Hf55ZXUmL99Cq97j+XnR79EOS3KgUBNGgru/5+6Hgtf7gApoi0SImXFr/QoM6dqIUoXy0OmDZP42aB57D2hAXCIn1ISxxcxuMbPY4HULsCWUjsH2s8xsSPB+vJnNDl6/mdm36fQ7nGq7wSHGKZKtVSlVkEH3NaBzk4p8MvVX2rw6nvnrdkQ7LMkhQk0Yd5JyS+0GYD1wLXB7iH27A4uOvnH3xu5ew91rAJOBb9Lpt/fodu7eLsR9iWR7ueNiebz1uXx610Xs2X+Yq16fqAJNEhEhJQx3X+3u7dw9wd1Lunt74JoT9QsGxtsAA9JYVwi4GEjzDENEjq9B5RKM6NGYy6qV4vnhi7l5wBR+U4EmCaPTqbj3YAjb9AYeAdKairM98HPwBHla8phZsplNMbP26e3AzDoH2yVv2rQphJBEso8i+XLR76ZavHjtBcxbm1Kg6XsVaJIwOZ2Ecdybwc2sLbDR3Weks8mNwGfH+Yjywb3BNwG9zaxSWhu5e393T3L3pIQEjcNLzmNmXJdUjmHdG1MxoQBdP5vFg1+oQJNkvNNJGCe6YNoQaGdmq4CBwMVm9jGAmZUA6gJD0/1w93XBzxXAGKDmacQqku2VL56fL7vUp9slVfh21jpa9x3PjNUq0CQZ57gJw8x2mdnONF67SHkeI13u/pi7J7p7BaADMMrdbwlWXwsMcfc0n0Ays6KpaoiXICX5LDy5X00k54mPjeHBy87myy71Abjuzcm8PPIXDqlAk2SA4yYMdy/o7oXSeBV097jT2G8HjrkcZWZJZnZ0cPxcINnM5gCjgefdXQlDJES1yxdjWLfGtK9Zlr4/L+XaNyezeosKNMnp0VxSItnckLm/8fg38zh8xHmqXXWura0CTfJfGT6XlIhkXW0vKMOIHk04P7EwD381l/s+ncn2PSrQJCdPCUMkByhTJC+f3FWPv7aqysiFv9Oy93gmLVOBJjk5ShgiOURsjNElKNCUL3csNw2Yyr+GLVKBJgmZEoZIDnNe2cIM7dqYmy86k/7jVnBVv0ks26gCTXJiShgiOVDeXLE8e9X5DLgtid937qNN3wl8NHkV2ekmGMl4ShgiOdil1UoxvEdj6lUszhPfLaDTB8ls2rU/2mFJJqWEIZLDlSyYh/fvqMM/2lVnwrLNtOw9jlGLVaBJ/kwJQ0QwMzo2SCnQlFAwN3e+n8wT385XgSb5H0oYIvIfZ5cqyHf3N+Qvjc/ioymrueK1CSrQJP+hhCEi/yN3XCx/a1ONjztdxK59B7nq9Ym8NXY5R1SgKcdTwhCRNDWqUoIR3ZtwSdVSPDd8Mbe8M5X1O1SgKSdTwhCRdBXNn4s3bqnFC9dcwOw122nZezxD566PdlgSJUoYInJcZsb1dcoxrFtjKpTIz32fzqTnl3PYvf9QtEOTCFPCEJGQVCiRn6+61KfbxZX5ZuZaWvcZz4zV26IdlkSQEoaIhCw+NoYHLz+Hz++uzxF3rn9rMr1/UoGmnEIJQ0ROWp0KxRjWvTFXXliG3j8t5fq3JvPrlj3RDkvCLOwJw8xizWyWmQ0J3r9vZivNbHbwqpFOv45mtjR4dQx3nCJycgrlieflG2rQ98aaLN24m9Z9x/PVjLWajyobi8QZRndg0TFtD7t7jeA1+9gOZlYMeBK4CKgLPGlmRcMfqoicrHYXphRoqlamED2/nMP9n85ix56D0Q5LwiCsCcPMEoE2wIATbXuMFsBId9/q7tuAkUDLjI5PRDJG2SJ5+ewv9Xik5Tn8sGADLfuMY9JyFWjKbsJ9htEbeAQ4dkTsWTOba2avmFnuNPqVBdaker82aPsTM+tsZslmlrxp06YMCVpETl5sjHFvs8oMurcheeNjuXnAVJ4bvogDhzQgnl2ELWGYWVtgo7vPOGbVY0BVoA5QDHj0dPbj7v3dPcndkxISEk7no0QkA5yfWJgh3RpxY90zeWvsCq56faIKNGUT4TzDaAi0M7NVwEDgYjP72N3Xe4r9wHukjFEcax1QLtX7xKBNRLKAfLni+NdV59P/1tqs37GPtq9O4KMpqzUgnsWFLWG4+2PunujuFYAOwCh3v8XMSgOYmQHtgflpdP8BuNzMigaD3ZcHbSKShVxe/QxG9GhM3bOK88S387nrg2Q271aBpqwqGs9hfGJm84B5QAngGQAzSzKzAQDuvhX4JzA9eD0dtIlIFlOyYB7ev70OT15RjfFBgabRizdGOyw5BZadThGTkpI8OTk52mGISDqWbNhF94GzWLxhF7fVL8/jrc8lT3xstMPK0cxshrsnhbKtnvQWkYg554yCfHtfQzo1OosPJ6/milcnsOA3FWjKKpQwRCSi8sTH8kTbanzUqS479h7kqn6TeHvcChVoygKUMEQkKhpXSWBEjyY0r5rAs8MWceu7U9mwY1+0w5LjUMIQkagplj8Xb95Sm+evPp+Zq7fTovc4hs9TgabMSglDRKLKzOhQ90yGdW9MheL5uOeTmTzy1Rz+UIGmTEcJQ0QyhbNK5Oerexpwf/PKfDVjLa37jmfWryrQlJkoYYhIphEfG0PPFucwsHN9Dh12rn1zMn1+WqoCTZmEEoaIZDp1zyrG8B6NueKC0rzy0y/c0H8Ka7aqQFO0KWGISKZUKE88vTvUpE+HGvyyYRet+oznm5kq0BRNShgikqldWaMsw3s0plrpQjz4xRy6fqYCTdGihCEimV5i0Xx81rkeD7c4hxHzN9CqzzgmL98S7bByHCUMEckSYmOM+5pX5ut7GpA7PpabBkzh3yMWq0BTBClhiEiWcmG5Igzt1ogOdcrxxpjlXP3GRJZv2h3tsHIEJQwRyXLy5Yrjuasv4K1ba7Nu217a9B3PxyrQFHZKGCKSZbWofgYjejShToVi/P3b+fzlw2S2qEBT2ChhiEiWVqpQHj64oy5PtK3GuKWbadF7PGOWqEBTOIQ9YZhZrJnNMrMhwftPzGyJmc03s3fNLD6dfofNbHbwGhzuOEUk64qJMTo1OovB9zekeP5c3P7edJ4avIB9Bw9HO7RsJRJnGN2BRanefwJUBc4H8gJ3pdNvr7vXCF7twhyjiGQDVc8oxHf3N+SOhhV4f9Iq2r02gYW/7Yx2WNlGWBOGmSUCbYABR9vcfZgHgGlAYjhjEJGcJU98LE9eUZ0P7qzLtj0Had9vIgPGq0BTRgj3GUZv4BHgTzdKB5eibgVGpNM3j5klm9kUM2uf3g7MrHOwXfKmTZsyJGgRyfqanp3ADz2a0PScBJ4Zuojb3p3G7ztVoOl0hC1hmFlbYKO7z0hnk9eBce4+Pp315YPC5DcBvc2sUlobuXt/d09y96SEhITTD1xEso1i+XPR/9baPHf1+cxYvY05sTIjAAANyUlEQVQWvccxYr4KNJ2qcJ5hNATamdkqYCBwsZl9DGBmTwIJwIPpdXb3dcHPFcAYoGYYYxWRbMrMuLHumQzt1ohyRfPR5eOZPPrVXBVoOgVhSxju/pi7J7p7BaADMMrdbzGzu4AWwI3unuYz/WZW1MxyB8slSEk+C8MVq4hkfxUTCvD1PQ24t1klvpixhjZ9xzN7zfZoh5WlROM5jDeBUsDk4JbZ/wMwsyQzOzo4fi6QbGZzgNHA8+6uhCEipyVXXAyPtKzKwL/U4+Bh55o3JvHaqKUc1oB4SCw7PUqflJTkycnJ0Q5DRLKAHXsP8sS38xk85zfqVCjKy9fXoFyxfNEOK+LMbEYwXnxCetJbRHKkwnnj6XtjTXrfUIPF63fRus94vp21LtphZWpKGCKSo7WvWZZh3RtTtXRBenw+m26fzWLHXhVoSosShojkeOWK5WNg5/r0vPxshs5bT+s+45m6QgWajqWEISJCSoGm+y+uwtf3NCA+1ujw9hReUIGm/6GEISKSSo1yRRjarTHX1y7H62OWc80bk1SgKaCEISJyjPy54/j3tRfw5i21WLNtD237TuCzab/m+AJNShgiIuloeV5pfujRhNrli/LYN/Po/NEMtv5xINphRY0ShojIcZQqlIcP76zL39ucy9glm2jRexxjf8mZE50qYYiInEBMjHFX44p8e19DiuaLp+O70/jH9zmvQJMShohIiKqVKcTg+xtxe4MKvDdxFVe+NpHFG3JOgSYlDBGRk5AnPpan2lXn/TvqsOWPA7R7bSLvTFiZIwo0KWGIiJyCZueU5IcejWlSpQT/HLKQju9l/wJNShgiIqeoeIHcvH1bEs9edR7TV22lZe9x/LBgQ7TDChslDBGR02Bm3HxReYZ0bUzZonm5+6MZPPbNXPYcyH4FmpQwREQyQOWSBfjmnobc06wSA6evoU3fCczJZgWalDBERDJIrrgYHm1ZlU/vqsf+g4e55o1J9Bu9LNsUaAp7wjCzWDObZWZDgvdnmdlUM1tmZp+bWa50+j0WbLPEzFqEO04RkYxSv1JxhndvQsvzzuDFH5ZwY/8prN22J9phnbZInGF0Bxalev9v4BV3rwxsAzod28HMqpFSB7w60BJ43cxiIxCriEiGKJwvnldvrMnL11/IwvU7adVnPN/NztoFmsKaMMwsEWgDDAjeG3Ax8FWwyQdA+zS6XgkMdPf97r4SWAbUDWesIiIZzcy4ulYiw7s35uxSBek+cDbdB85i576sWaAp3GcYvYFHgKMTyhcHtrv70dsH1gJl0+hXFliT6n1624mIZHrliuXj8871ePCysxkydz2teo9n2sqt0Q7rpIUtYZhZW2Cju88I1z6C/XQ2s2QzS960KWdOCCYimV9cbAzdLqnCV13qExdrdOg/mZd+WMLBw1mnQFM4zzAaAu3MbBUwkJRLUX2AImYWF2yTCKR1UW8dUC7V+/S2w937u3uSuyclJCRkVOwiImFR88yiDO3WmGtrJ/La6GVc+8YkVm7+I9phhSRsCcPdH3P3RHevQMoA9ih3vxkYDVwbbNYR+C6N7oOBDmaW28zOAqoA08IVq4hIJBXIHccL117IGzfXYtWWPbTuM56BWaBAUzSew3gUeNDMlpEypvEOgJm1M7OnAdx9AfAFsBAYAdzn7jlrHmERyfZanV+aET0aU6t8Ef76zTy6fDyDbZm4QJNl9ox2MpKSkjw5OTnaYYiInJQjR5x3JqzkxR+WUCRfPL2uv5DGVSJzid3MZrh7Uijb6klvEZEoi4kx/tKkIoPua0ChvPHc+s40nv5+YaYr0KSEISKSSVQvU5ghXRvRsX553p24kvb9JrJkw65oh/UfShgiIplInvhY/nHlebx3ex02797PFa9N4L2JKzPFgLgShohIJtS8aklG9GhC48ol+Mf3C+n43nQ2RrlAkxKGiEgmVaJAbgZ0TOKf7c9j2sottOwznh+jWKBJCUNEJBMzM26tV54hXRtRunAeOn80g8cHzYtKgSYlDBGRLKByyYIMurchdzetyGfTfqVt3wnMW7sjojEoYYiIZBG54mJ4rNW5fHLXRew9eJirXp8Y0QJNShgiIllMg0olGNG9CS2qBwWa3p7CH/vDf4kq7sSbiIhIZlM4Xzyv3VST5jNLMn3lVvLlCn+NOSUMEZEsysy4tnYi19ZOjMj+dElKRERCooQhIiIhUcIQEZGQKGGIiEhIlDBERCQkShgiIhISJQwREQmJEoaIiIQkW9X0NrNNwOpT7F4C2JyB4WQUxXVyFNfJUVwnJzvGVd7dQyognq0Sxukws+RQC6FHkuI6OYrr5Ciuk5PT49IlKRERCYkShoiIhEQJ47/6RzuAdCiuk6O4To7iOjk5Oi6NYYiISEh0hiEiIiFRwhARkZBk+4RhZi3NbImZLTOzv6axPreZfR6sn2pmFVKteyxoX2JmLSIc14NmttDM5prZz2ZWPtW6w2Y2O3gNjnBct5vZplT7vyvVuo5mtjR4dYxwXK+kiukXM9ueal04j9e7ZrbRzOans97MrG8Q91wzq5VqXTiP14niujmIZ56ZTTKzC1OtWxW0zzaz5AjH1czMdqT68/q/VOuO+x0Ic1wPp4ppfvCdKhasC+fxKmdmo4N/CxaYWfc0toncd8zds+0LiAWWAxWBXMAcoNox29wLvBksdwA+D5arBdvnBs4KPic2gnE1B/IFy/ccjSt4vzuKx+t24LU0+hYDVgQ/iwbLRSMV1zHbdwXeDffxCj67CVALmJ/O+tbAcMCAesDUcB+vEONqcHR/QKujcQXvVwElonS8mgFDTvc7kNFxHbPtFcCoCB2v0kCtYLkg8Esafycj9h3L7mcYdYFl7r7C3Q8AA4Erj9nmSuCDYPkr4BIzs6B9oLvvd/eVwLLg8yISl7uPdvc9wdspQCRqMIZyvNLTAhjp7lvdfRswEmgZpbhuBD7LoH0fl7uPA7YeZ5MrgQ89xRSgiJmVJrzH64RxufukYL8Que9XKMcrPafz3czouCL5/Vrv7jOD5V3AIqDsMZtF7DuW3RNGWWBNqvdr+fPB/s827n4I2AEUD7FvOONKrRMp/4M4Ko+ZJZvZFDNrn0ExnUxc1wSnvl+ZWbmT7BvOuAgu3Z0FjErVHK7jFYr0Yg/n8TpZx36/HPjRzGaYWecoxFPfzOaY2XAzqx60ZYrjZWb5SPlH9+tUzRE5XpZyubwmMPWYVRH7jsWdTmcJPzO7BUgCmqZqLu/u68ysIjDKzOa5+/IIhfQ98Jm77zezu0k5O7s4QvsORQfgK3c/nKotmscrUzOz5qQkjEapmhsFx6skMNLMFgf/A4+EmaT8ee02s9bAt0CVCO07FFcAE9099dlI2I+XmRUgJUn1cPedGfnZJyO7n2GsA8qlep8YtKW5jZnFAYWBLSH2DWdcmNmlwN+Adu6+/2i7u68Lfq4AxpDyv46IxOXuW1LFMgCoHWrfcMaVSgeOuVwQxuMVivRiD+fxComZXUDKn+GV7r7laHuq47URGETGXYo9IXff6e67g+VhQLyZlSATHK/A8b5fYTleZhZPSrL4xN2/SWOTyH3HwjFQk1lepJxBrSDlEsXRgbLqx2xzH/876P1FsFyd/x30XkHGDXqHEldNUgb5qhzTXhTIHSyXAJaSQYN/IcZVOtXyVcAU/+8A28ogvqLBcrFIxRVsV5WUAUiLxPFKtY8KpD+I24b/HZCcFu7jFWJcZ5IyLtfgmPb8QMFUy5OAlhGM64yjf36k/MP7a3DsQvoOhCuuYH1hUsY58kfqeAW/+4dA7+NsE7HvWIYd7Mz6IuUOgl9I+cf3b0Hb06T8rx0gD/Bl8JdnGlAxVd+/Bf2WAK0iHNdPwO/A7OA1OGhvAMwL/sLMAzpFOK7ngAXB/kcDVVP1vTM4jsuAOyIZV/D+KeD5Y/qF+3h9BqwHDpJyjbgT0AXoEqw3oF8Q9zwgKULH60RxDQC2pfp+JQftFYNjNSf4c/5bhOO6P9X3awqpElpa34FIxRVsczspN8Kk7hfu49WIlDGSuan+rFpH6zumqUFERCQk2X0MQ0REMogShoiIhEQJQ0REQqKEISIiIVHCEBGRkChhiJzAMbPdzs7ImVLNrEJ6M6SKZDaaGkTkxPa6e41oByESbTrDEDlFQR2EF4JaCNPMrHLQXsHMRtl/a5mcGbSXMrNBwcR6c8ysQfBRsWb2dlDv4Eczyxts383+WxNlYJR+TZH/UMIQObG8x1ySuiHVuh3ufj7wGtA7aHsV+MDdLwA+AfoG7X2Bse5+ISm1FxYE7VWAfu5eHdgOXBO0/xWoGXxOl3D9ciKh0pPeIidgZrvdvUAa7auAi919RTBB3AZ3L25mm0mZc+tg0L7e3UuY2SYg0VNNJBlMWT3S3asE7x8F4t39GTMbAewmZcbWbz2YlE8kWnSGIXJ6PJ3lk7E/1fJh/ju22IaUOYJqAdOD2ZRFokYJQ+T03JDq5+RgeRIpMx8D3AyMD5Z/JqXcLmYWa2aF0/tQM4sByrn7aOBRUmZK/dNZjkgk6X8sIieW18xmp3o/wt2P3lpb1MzmknKWcGPQ1hV4z8weBjYBdwTt3YH+ZtaJlDOJe0iZITUtscDHQVIxoK+7b8+w30jkFGgMQ+QUBWMYSe6+OdqxiESCLkmJiEhIdIYhIiIh0RmGiIiERAlDRERCooQhIiIhUcIQEZGQKGGIiEhI/h8fyT15fyDYhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def eval_step(img_tensor, target):\n",
    "#     \"\"\"\n",
    "#     basically same as train_step, but doesn't apply gradient \n",
    "#     \"\"\"\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     # initializing the hidden state for each batch\n",
    "#     # because the captions are not related from image to image\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']] * target.shape[0], 1)\n",
    "    \n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         features = encoder(img_tensor)\n",
    "\n",
    "#         for i in range(1, target.shape[1]):\n",
    "            \n",
    "#             # passing the features through the decoder\n",
    "#             predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "\n",
    "#             loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "#             # using teacher forcing\n",
    "#             decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "#     total_loss = (loss / int(target.shape[1]))\n",
    "    \n",
    "#     return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256# ## Customized to enable multiple images, and incorporate PPLM framework\n",
    "\n",
    "\n",
    "# def custom_evaluate(image, supporting_images):\n",
    "    \n",
    "#     attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "#     # initializing the hidden state for decoder\n",
    "#     hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     # Extract image features\n",
    "#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "#     features = encoder(img_tensor_val)\n",
    "    \n",
    "    \n",
    "#     ## TODO: extract features from supporting images\n",
    "#     ## TODO: average those features into single context vector\n",
    "    \n",
    "    \n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     ## TODO: BERT doesn't have word_index\n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']], 0)\n",
    "#     result = []\n",
    "\n",
    "#     for i in range(max_length):\n",
    "#         \"\"\"\n",
    "#         Currently using inject method \n",
    "#         (image feature injected at every timestep)\n",
    "        \n",
    "#         TODO: test merge approach & pre, par inject\n",
    "#         \"\"\"\n",
    "        \n",
    "#         ## image feature inject's method\n",
    "#         predictions, hidden, attention_weights = decoder(decoder_input, features, hidden)\n",
    "        \n",
    "        \n",
    "#         ## TODO : apply PPLM here\n",
    "#         ## check loss (prediction - context vector of supporting images)\n",
    "#         ## apply gradient : hidden_state += diffrence(pred, supporting img vectors) (after n-iteration)\n",
    "#         ## re compute predictions\n",
    "        \n",
    "        \n",
    "#         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        \n",
    "        \n",
    "#         ## TODO: Allow using BERT tokenizer\n",
    "#         ## TODO: Revert predictions to index and words\n",
    "#         result.append(tokenizer.index_word[predicted_id])   # <<< return back id to text\n",
    "\n",
    "        \n",
    "#         if tokenizer.index_word[predicted_id] == '[END]':\n",
    "#             return result, attention_plot\n",
    "\n",
    "#         decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result), :]\n",
    "#     return result, attention_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
