{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 60\n",
    "\n",
    "\n",
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"START\" + annot['caption']\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "train_captions = all_captions[:NUM_SAMPLES]\n",
    "train_img_paths = all_img_paths[:NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 60\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"xception\"\n",
    "\n",
    "\n",
    "def get_encoder(model_type=MODEL_TYPE):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.keras.applications.xception\n",
    "        cnn_model = tf.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "encoder, encoder_preprocessor = get_encoder(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = encoder_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "# Get unique images\n",
    "unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# Prepare dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 8.5\n"
     ]
    }
   ],
   "source": [
    "estimated_batch_count = NUM_SAMPLES / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:02,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed image (batch)\n",
    "for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "    # get context vector of batch images\n",
    "    batch_features = encoder(batch_imgs)\n",
    "    \n",
    "    # flatten 2D cnn result into 1D for RNN decoder input\n",
    "    # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    # Cache preprocessed image\n",
    "    for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "        image_path = image_path.numpy().decode(\"utf-8\")\n",
    "        np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"BERT\"\n",
    "VOCAB_SIZE = 5000  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(x) for x in texts]\n",
    "        token_ids = [self.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        \n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOKENIZER == \"BERT\" :\n",
    "\n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "else : \n",
    "    \n",
    "    # use default keras tokenizer\n",
    "    tokenizer = TokenizerWrapper(num_words=VOCAB_SIZE, oov_token=\"[UNK]\")\n",
    "    tokenizer.fit_on_texts(train_captions)    \n",
    "    tokenizer.word_index['[PAD]'] = 0\n",
    "    tokenizer.index_word[0] = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "MAX_LENGTH = None  # use <int> or None\n",
    "\n",
    "\n",
    "train_captions = pad_sequences(train_captions, maxlen=MAX_LENGTH, padding='post', truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def load_image_npy(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "from tensorflow.data import Dataset\n",
    "dataset = Dataset.from_tensor_slices((train_img_paths, train_captions))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          load_image_npy, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(NUM_SAMPLES / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 8, 100, 2048)\n",
    "# caption => (batch_size = 8, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5 batches, (total : 40)\n",
      "eval : 1 batches, (total : 8)\n",
      "test : 2 batches, (total : 16 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2707  2319  2214  3902  5013  2075  2091  1996  2346  2007  1996  4062\n",
      "   5710  1012     0     0     0     0     0     0]\n",
      " [ 2707  5280 16446  1998  1037  3526  3042 10201  2006  1037  2795  1012\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 2707  2050  5629  2158  2007  1037 13555  4402  7719  1999  1996 19978\n",
      "   1997  1037  4316  5129  2011 17434  1012     0]\n",
      " [ 2707  2050  5119  2003  2328  2046  1996  2217  1997  1996 28799  4330\n",
      "   3578  1012     0     0     0     0     0     0]\n",
      " [ 2707  2050  5127  1997  3059  9440  3139  1999  8808  2015     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 2707  2050  5723  2007  1037 11848  1998 12509  2121  4987  2000  1996\n",
      "   2813  1012     0     0     0     0     0     0]\n",
      " [ 2707  5051 27469  2551  2006 12191  7588  1999  1037  3076  7759  2282\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 2707  2102 12155  5055  2379  2028  2178  1999  1037  2492     0     0\n",
      "      0     0     0     0     0     0     0     0]], shape=(8, 20), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset:\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam()\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    real => (batch_size = 8, (1))\n",
    "    pred => (batch_size = 8, vocab_size)\n",
    "    mask => Tensor(\"Cast:0\", shape=(batch_size,), dtype=float32)\n",
    "    loss_ => Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(batch_size,), dtype=float32)\n",
    "\n",
    "    return => Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, 64, embedding_dim)\n",
    "        hidden                        => (batch_size, hidden_size = ??)\n",
    "        \n",
    "        hidden_with_time_axis => (batch_size, 1, hidden_size)\n",
    "        score                 => (batch_size, 64, hidden_size)\n",
    "        attention_weights     => (batch_size, 64, 1)\n",
    "        context_vector (after sum) => (batch_size, hidden_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, 64, output_dim)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "\n",
    "WORD_EMBEDDING = \"BERT\"\n",
    "RNN_TYPE = \"LSTM\"\n",
    "STRATEGY = \"inject\"\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=32, embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 strategy=\"inject\", vocab_size=5000):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.strategy = strategy\n",
    "        \n",
    "\n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        else:\n",
    "            self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.rnn = LSTM(self.rnn_units,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.rnn = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "           \n",
    "        \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(vocab_size) # same size as vocab\n",
    "\n",
    "        # attention layer for image (CNN encoder output)\n",
    "        self.attention = BahdanauAttention(self.rnn_units)\n",
    "        \n",
    "        \n",
    "    def get_embedding(self, x):\n",
    "        if self.embedding_type == \"BERT\": \n",
    "            return self._get_bert_embedding(x)\n",
    "        else:\n",
    "            return self._get_default_embedding(x)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _get_bert_embedding(self, x, output_layer=11):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \n",
    "        x (Text tokens) => (batch_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Format as torch Tensor\n",
    "        x = torch.as_tensor(x.numpy())\n",
    "        x = x.type(torch.LongTensor)\n",
    "        \n",
    "        # BERT's embedding\n",
    "        with torch.no_grad():\n",
    "            embedding , _ = self.bert_model(x)\n",
    "\n",
    "        # Revert back to tf.Tensor\n",
    "        x = embedding[output_layer].numpy()\n",
    "        x = tf.convert_to_tensor(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def _get_default_embedding(self, x):\n",
    "        \"\"\"\n",
    "        Get default embedding for text tokens\n",
    "        \n",
    "        x (Text tokens) => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "        \n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        \"\"\" \n",
    "        this using inject method,\n",
    "        image's feature injected at every timestep \n",
    "        \n",
    "        x : decoder input, also last word generated => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x1 => (batch_size, 1, embedding_dim)\n",
    "        x1 = self.get_embedding(x)\n",
    "\n",
    "        \n",
    "        ## TODO: inject image here ?\n",
    "        ## TODO: add PPLM framework here\n",
    "        ## feed-forward, find gradient, apply gradient, feed-forward again \n",
    "        \n",
    "\n",
    "        ## Concat context vector + hidden state => inject method\n",
    "        ## TODO : use merge method, pre method\n",
    "        \n",
    "        if self.strategy == \"inject\":\n",
    "            # x2 => (batch_size, 1, image_feature_size + embedding_dim)\n",
    "            x2 = tf.concat([tf.expand_dims(context_vector, 1), x1], axis=-1)\n",
    "        else:\n",
    "            x2 = x1\n",
    "            \n",
    "            \n",
    "        # passing the concatenated vector to the RNN (LSTM / GRU)\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, h_state, c_state = self.rnn(x2)\n",
    "            state = h_state\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, state = self.rnn(x2)\n",
    "        \n",
    "\n",
    "        ## ======================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ======================\n",
    "                \n",
    "        if self.strategy == \"merge\":\n",
    "            \n",
    "            # output => (batchsize = 8, 1, image_feature_size + embedding_dim)\n",
    "            output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
    "            \n",
    "            # if using add instead of concat\n",
    "            # output = tf.keras.layers.add([context_vector, output])\n",
    "\n",
    "            \n",
    "        # x3 shape => (batch_size, 1, rnn_units = 32)\n",
    "        x3 = self.fc1(output)\n",
    "\n",
    "        # x4 => (batch_size, rnn_units = 32)\n",
    "        x4 = tf.reshape(x3, (-1, x3.shape[2]))\n",
    "\n",
    "        \n",
    "        ## TODO : merge method, concat context vector + hiddens state here .\n",
    "        \n",
    "        \n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        print(\"word_predictions\", word_predictions.shape)\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "        return word_predictions, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.rnn_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "\n",
    "def train_step(img_tensor, target):\n",
    "    \n",
    "    loss = 0\n",
    "    batch_size = target.shape[0]\n",
    "    \n",
    "    print(\"TARGET\")\n",
    "    print(type(target))\n",
    "#     target = torch.Tensor(list(target))\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "    ## decoder_input == last word generated\n",
    "    decoder_input = tf.expand_dims(tokenizer.convert_tokens_to_ids(['start']) * batch_size, 1)\n",
    "\n",
    "#     decoder_input = torch.tensor([tokenizer.convert_tokens_to_ids(['start'])] * batch_size)\n",
    "   \n",
    "    print(\"decoder input\")\n",
    "    print(type(decoder_input))\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        ## Get image context vector\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            \n",
    "            print(\"LOOP : \", i)\n",
    "            \n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "            \n",
    "            # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "            print(\"decoder_input\", decoder_input.shape)\n",
    "            \n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FEATURE_DIM = 256\n",
    "WORD_EMBEDDING_DIM = 768 # BERT's embedding dim\n",
    "UNITS = 32\n",
    "VOCAB_SIZE = len(tokenizer.vocab.keys()) # BERT's vocab size\n",
    "\n",
    "encoder = CNN_Encoder(\n",
    "    output_dim=IMAGE_FEATURE_DIM\n",
    ")\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=\"LSTM\",\n",
    "    rnn_units=UNITS,\n",
    "    embedding_type=\"BERT\",\n",
    "    embedding_dim=WORD_EMBEDDING_DIM,  \n",
    "    vocab_size=VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "decoder input\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "LOOP :  1\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  2\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  3\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  4\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  5\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  6\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  7\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  8\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  9\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  10\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  11\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  12\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  13\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  14\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  15\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  16\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  17\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  18\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  19\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "TARGET\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "decoder input\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "LOOP :  1\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  2\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  3\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  4\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  5\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  6\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  7\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  8\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  9\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  10\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  11\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  12\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  13\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  14\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  15\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  16\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  17\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  18\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  19\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "TARGET\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "decoder input\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "LOOP :  1\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  2\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  3\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  4\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  5\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  6\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  7\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  8\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  9\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  10\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  11\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  12\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  13\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  14\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  15\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  16\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  17\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  18\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  19\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "TARGET\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "decoder input\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "LOOP :  1\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  2\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  3\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  4\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  5\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  6\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  7\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  8\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  9\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  10\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  11\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  12\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  13\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  14\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  15\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  16\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  17\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  18\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  19\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "TARGET\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "decoder input\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "LOOP :  1\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  2\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  3\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  4\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  5\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  6\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  7\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  8\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  9\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  10\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  11\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  12\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  13\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  14\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  15\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  16\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  17\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  18\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n",
      "LOOP :  19\n",
      "word_predictions (8, 30522)\n",
      "\n",
      "\n",
      "decoder_input (8, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=239214, shape=(), dtype=float32, numpy=30.511932>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss = 0\n",
    "\n",
    "for img_tensor, target in train_dataset:\n",
    "    batch_loss, t_loss = train_step(img_tensor, target)\n",
    "    total_loss += t_loss\n",
    "    \n",
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "decoder input\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "LOOP :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_get_bert_embedding() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-08bd7feb08d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-259-62c2810e165d>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(img_tensor, target)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# passing the features through the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# loss => Tensor(\"add:0\", shape=(), dtype=float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-258-76e8d6694e6f>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, features, hidden)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# x1 = self.embedding(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# ===================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _get_bert_embedding() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset)):\n",
    "        \n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            \n",
    "        \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd4VWXW/vHvSqF3CIiEXkSKIIYOQRmaqIDK2FDRQVERCxkddeZ1xjbO6PyGplhQVKygWEBUmiIBpIXeRAMCgigBKSId1u+Ps3kn4xtIIDk5Kffnus7FOXvvZ5/1jDK3u5y1zd0RERE5U1GRLkBERPI3BYmIiGSLgkRERLJFQSIiItmiIBERkWxRkIiISLYoSETyETO70My2RLoOkfQUJCInYWYbzaxLBL73JjM7Zmb7zGyvmS0zs0vPYD+vmdkT4ahRJD0FiUjeNM/dSwHlgDHAu2ZWPsI1iWRIQSJyBszsVjNLNbOfzWySmZ0dLDczG2Zm24OjiZVm1iRY19PM1pjZL2a21czuy+x73P048ApQHKibQR3nmtmXZrbbzFabWa9g+UCgH/Cn4Mjm4xycvsh/UZCInCYz6wz8A7gKqApsAsYFq7sBiUADoGywzc5g3RjgNncvDTQBvsjCd8UAtwD7gG9/sy4W+BiYBlQG7gLeMrNz3H008BbwtLuXcvfLznjCIplQkIicvn7AK+6+xN0PAQ8Bbc2sFnAEKA00BMzd17r7tmDcEaCRmZVx913uvuQU39HGzHYDPwLXApe7+57fbgOUAv7p7ofd/QtgcrC9SK5RkIicvrMJHYUA4O77CB11VAv+z/xZYBSw3cxGm1mZYNMrgZ7AJjObZWZtT/Ed8929nLtXcvc27j7jJHV8H5z+OmETUO3MpyZy+hQkIqfvB6DmiQ9mVhKoCGwFcPeR7n4B0IjQKa77g+WL3L03odNQHwHv5kAd1c0s/d/jGifqANTaW3KFgkTk1GLNrFi6VwzwDnCzmTU3s6LAk8ACd99oZi3NrHVw/eJX4CBw3MyKmFk/Myvr7keAvcDxk35r1iwA9hO6oB5rZhcCl/Gf6zU/AXWy+R0imVKQiJzap8CBdK9HgtNMDwPvA9sI3U11TbB9GeAlYBeh00w7gX8F624ANprZXuB2Qtdazpi7HyYUHBcDO4DngBvd/etgkzGErsnsNrOPsvNdIqdierCViIhkh45IREQkWxQkIiKSLQoSERHJFgWJiIhkS0ykC8gNlSpV8lq1akW6DBGRfGXx4sU73D0us+0KRZDUqlWLlJSUSJchIpKvmNmmzLfSqS0REckmBYmIiGSLgkRERLJFQSIiItkStiAJGtwtNLPlwZPbHg2W1zazBcHT5cabWZEMxtYyswPBs6qXmdkL6dZdEDx1LtXMRpqZhWsOIiKSuXAekRwCOrt7M6A50MPM2gBPAcPcvR6hxnYDTjJ+vbs3D163p1v+PHArUD949QjbDEREJFNhCxIP2Rd8jA1eDnQGJgTLxwJ9srpPM6sKlHH3+R7qNvn66YwXEZGcF9ZrJGYWbWbLgO3AdGA9sNvdjwabbOHkT3OrbWZLgyfJdQyWVQvGnHDS8WY20MxSzCwlLS3tjOp/fd5GZn1zZmNFRAqLsAaJux9z9+ZAPNCK0HOss2IbUMPdzweSgLfTPa40q9892t0T3D0hLi7TH2b+H0eOHeftBZvp/8pC/vjucnbvP3za+xARKQxy5a4td98NzATaAuWCp8xBKGC2ZrD9IXffGbxfTOhIpkGwbXy6TTMcnxNio6P46M72DL6oHh8t20qXocl8tnJbOL5KRCRfC+ddW3FmVi54XxzoCqwlFCh9g836AxNPMjY6eF+H0EX1De6+DdhrZm2Cu7VuzGh8TikWG8193c9h0uD2VClTlDveWsLtbyxm+96D4fpKEZF8J5xHJFWBmWa2AlgETHf3ycADQJKZpQIVCT0OFDPrZWaPBWMTgRXB9ZUJwO3u/nOwbhDwMpBK6EjlszDOAYDGZ5dl4p3teaBHQ75Yt50uQ2fxbsr36OmSIiKF5FG7CQkJnlNNG9en7ePB91ewaOMuOtavxJOXN6V6hRI5sm8RkbzEzBa7e0Jm2+mX7aepblwpxg9sy+O9G7Nk0y66D0/m1bnfcex4wQ9kEZGMKEjOQFSUcUPbWkxL6kTLWhV49OM1XPXiPFK3/xLp0kREcp2CJBuqlSvOaze3ZOhVzVifto+eI+bw7BffcuTY8UiXJiKSaxQk2WRmXNEinulDOtG1cRX+37Rv6PXsXFZu2RPp0kREcoWCJIfElS7KqOta8OINF7Bj3yH6PDeXf372NQePHIt0aSIiYaUgyWHdG5/FjCGd6NsinhdmrefiEbNZsGFnpMsSEQkbBUkYlC0Ry1N9z+PNAa05cuw4V4+ez8MfreKXg0ciXZqISI5TkIRRh/qVmDYkkT+0r82bCzbRfVgyM9dtj3RZIiI5SkESZiWKxPDXyxrx/h3tKFk0hptfXcSQ8cv4+Vc1gRSRgkFBkkta1CjP5Ls7cHfneny8/Ae6Dp3F5BU/qM2KiOR7CpJcVDQmmqRu5/DxXR04u1xxBr+9lIFvLOYnNYEUkXxMQRIB51Ytw4eD2vHQxQ1J/iaNLkNnMX7RZh2diEi+pCCJkJjoKG7rVJcp9yZybtUyPPD+Svq9vIDNO/dHujQRkdOiIImw2pVKMu7WNvz98ias2LKH7sOTGTNHTSBFJP9QkOQBUVFGv9Y1mZ6USNu6FXl88hqufP4rvvlJTSBFJO9TkOQhVcsWZ0z/BEZc05xNO3/lkpGzGTHjWw4fVRNIEcm7FCR5jJnRu3k1ZiR1okeTqgyb8Q29np3D8u93R7o0EZEMhfOZ7cXMbKGZLTez1Wb2aLC8tpktMLNUMxtvZkVOsY8aZrbPzO5Lt2yjma00s2VmljOPPcyDKpYqyjPXns9LNyawa/9hLn9uLk9+upYDh9UEUkTylnAekRwCOrt7M6A50MPM2gBPAcPcvR6wCxhwin0MJeNnsl/k7s2z8gjI/K5roypMT+rE1S1rMDp5AxePSGbeejWBFJG8I2xB4iH7go+xwcuBzsCEYPlYoE9G482sD/AdsDpcNeYXZYrF8o8rmvL2ra1x4NqX5vPnD1eyV00gRSQPCOs1EjOLNrNlwHZgOrAe2O3uR4NNtgDVMhhXCngAeDSD3TowzcwWm9nA8FSeN7WrW4kp9yRya8fajFu4mW5Dk/l87U+RLktECrmwBom7H3P35kA80ApomMWhjxA6/bUvg3Ud3L0FcDFwp5klZrQDMxtoZilmlpKWlnYG1edNxYtE85dLGvHBoPaULR7LgLEp3P3OUnbuOxTp0kSkkMqVu7bcfTcwE2gLlDOzmGBVPLA1gyGtgafNbCNwL/BnMxsc7Gtr8Od24ENCAZXRd4529wR3T4iLi8vJ6eQJzauX4+O7OnBvl/p8tmobXYclM3HZVrVZEZFcF867tuLMrFzwvjjQFVhLKFD6Bpv1Byb+dqy7d3T3Wu5eCxgOPOnuz5pZSTMrHeyzJNANWBWuOeR1RWKiuLdLAybf1ZHqFUpwz7hl3DI2hW17DkS6NBEpRMJ5RFIVmGlmK4BFwHR3n0zo2keSmaUCFYExAGbWy8wey2SfVYA5ZrYcWAh84u5TwjaDfOKcs0rzwR3t+J9LzmXu+h10G5rM2ws2c1xtVkQkF1hhOBWSkJDgKSkF9icn/2XTzl958P2VzNuwkzZ1KvDPK86jVqWSkS5LRPIhM1uclZ9Z6JftBUzNiiV5+9bW/POKpqzeupceI5J5KXkDR4+pzYqIhIeCpAAyM65pVYPpSZ3oUK8Sf/90LVc+/xVf/7g30qWJSAGkICnAzipbjJduTOCZa89ny64DXDpyDkOnf8Oho2qzIiI5R0FSwJkZlzU7m+lJnbj0vKqM/PxbLntmDks374p0aSJSQChICokKJYsw/JrzeeWmBH45eJQrnv+KxyevYf/ho5kPFhE5BQVJIdO5YRWmDUmkX+sajJnzHT2Gz+ar1B2RLktE8jEFSSFUulgsT/RpyriBbYgyuO7lBTz4/gr2HFATSBE5fQqSQqxNnYpMuTeR2zrV4d2U7+k6dBbTVv8Y6bJEJJ9RkBRyxWKjeejic/nozvZUKFmEgW8sZvDbS9ihJpAikkUKEgHgvPhyTBrcgT92bcC01T/RZegsPly6RU0gRSRTChL5X0Viorjrd/X55O4O1K5UkiHjl/OH1xbxw241gRSRk1OQyP9Rv0ppJtzejr9e2oj5G36m27Bk3pi/SU0gRSRDChLJUHSU8YcOtZk2JJHm1cvx8EeruGb0fDakZfSsMREpzBQkckrVK5TgjQGtePrK81j7414uHjGbF2atVxNIEflfChLJlJlxVcvqzEjqRKcGcfzzs6/p89xc1vygJpAioiCR01ClTDFevOECnuvXgh/3HKTXs3P497R1agIpUsgpSOS0mBk9m1Zl+pBO9Gp+Ns98kcolI+eweJOaQIoUVgoSOSPlSxZh6FXNee3mlhw4fIy+L3zFox+v5tdDagIpUtiELUjMrJiZLTSz5Wa22sweDZbXNrMFZpZqZuPNrMgp9lHDzPaZ2X3plvUws3XB+AfDVb9kzYXnVGbqkERuaFOTV+dupPvwZGZ/mxbpskQkF4XziOQQ0NndmwHNgR5m1gZ4Chjm7vWAXcCAU+xjKPDZiQ9mFg2MAi4GGgHXmlmjMNUvWVSqaAyP9W7Cu7e1pUh0FDeMWcj97y1nz341gRQpDMIWJB5y4kcHscHLgc7AhGD5WKBPRuPNrA/wHbA63eJWQKq7b3D3w8A4oHcYypcz0Kp2BT69pyODLqzLB0u30mXYLKasUhNIkYIurNdIzCzazJYB24HpwHpgt7ufOJG+BaiWwbhSwAPAo79ZVQ34Pt3nDMcH+xhoZilmlpKWplMtuaVYbDR/6tGQiXe2J65UUW5/czGD3lrM9l8ORro0EQmTsAaJux9z9+ZAPKGjiYZZHPoIodNfZ/wzancf7e4J7p4QFxd3pruRM9SkWlkmDm7P/d3PYcba7XQdmsz7i9UEUqQgismNL3H33WY2E2gLlDOzmOCoJB7YmsGQ1kBfM3saKAccN7ODwGKgerrtTjZe8oDY6CjuvKge3RufxQPvr+CP7y1n4vIfePLyJsSXLxHp8kQkh4Tzrq04MysXvC8OdAXWAjOBvsFm/YGJvx3r7h3dvZa71wKGA0+6+7PAIqB+cOdXEeAaYFK45iA5o17lUrx3W1se7dWYlI2hJpBjv9qoJpAiBUQ4T21VBWaa2QpCATDd3ScTuvaRZGapQEVgDICZ9TKzx061w+AoZjAwlVAovevuq081RvKGqCijf7taTL03kQtqludvk1Zz1YvzWK8mkCL5nhWGc9YJCQmekpIS6TIk4O68v2Qrj09ew4Ejx7jnd/UZmFiH2Gj9PlYkLzGzxe6ekNl2+psruc7M6HtBPNOTEulybmX+NXUdfUbNZdXWPZEuTUTOgIJEIqZy6WI81+8CXri+BT/tPUTvUXN5esrXHDyiJpAi+YmCRCKuR5OqfJ7UiSvOr8ZzX66n54jZLNr4c6TLEpEsUpBInlC2RCz/+n0zXv9DKw4dPc7vX5jHXyeuYp+aQIrkeQoSyVMSG8QxbUgiN7WrxRvzN9F9WDKzvlFnApG8TEEieU7JojE80qsxE25vS7HYKPq/spCkd5exe//hSJcmIhlQkEiedUHNCnxyd0cGX1SPSct+oMvQWXy6clukyxKR31CQSJ5WLDaa+7qfw8TB7TmrbDEGvbWE295IYfteNYEUySsUJJIvND67LB8Nas8DPRoyc10aXYbO4t2U79UEUiQPUJBIvhETHcUdF9Zlyj0daXhWGf40YQU3jFnI9z/vj3RpIoWagkTynTpxpRg3sA2P92nC0s276DYsmVfnfscxNYEUiQgFieRLUVHGDW1qMi2pE63rVODRj9fw+xe+InX7L5EuTaTQUZBIvlatXHFevaklw65uxoYdv9JzxBye/eJbjhw7HunSRAoNBYnke2bG5efHMyOpE10bV+H/TfuGy56Zw8otagIpkhsUJFJgVCpVlFHXteDFGy7g518P03vUHP7x2Vo1gRQJMwWJFDjdG5/F9KROXJVQnRdnbeDiEbNZsGFnpMsSKbAUJFIglS0eyz+vPI+3bmnN0ePHuXr0fP7no5X8cvBIpEsTKXAUJFKgta9Xian3JjKgQ23eWrCZ7sOSmfn19kiXJVKghC1IzKyYmS00s+VmttrMHg2W1zazBWaWambjzaxIBmNbmdmy4LXczC5Pt26jma0M1un5uZKpEkViePjSRrx/RztKFo3h5tcWMWT8Mn7+VU0gRXJC2J7ZbmYGlHT3fWYWC8wB7gGSgA/cfZyZvQAsd/fnfzO2BHDY3Y+aWVVgOXB28HkjkODuO7Jai57ZLiccOnqMUTPX89zMVMoWj+WRXo259LyqhP51FZH0Iv7Mdg/ZF3yMDV4OdAYmBMvHAn0yGLvf3U880ahYME4k24rGRJPUtQEf39WBauWLc9c7S7n19cX8pCaQImcsrNdIzCzazJYB24HpwHpgd7qQ2AJUO8nY1ma2GlgJ3J5ujAPTzGyxmQ08xXcPNLMUM0tJS9ODkeS/nVu1DB/c0Y4/92zI7G9DTSDHLdysJpAiZyCsQeLux9y9ORAPtAIansbYBe7eGGgJPGRmxYJVHdy9BXAxcKeZJZ5k/Gh3T3D3hLi4uOxNRAqkmOgoBibWZeq9iTSqWoYHP1hJv5cXsHmnmkCKnI5cuWvL3XcDM4G2QDkziwlWxQNbMxm7FtgHNAk+bw3+3A58SCigRM5YrUoleefWNjx5eVNWbNlDt+GzeHn2BjWBFMmicN61FWdm5YL3xYGuwFpCgdI32Kw/MDGDsbVPhI2Z1SR0JLPRzEqaWelgeUmgG7AqXHOQwiMqyriudQ2mJyXSrm4lnvhkLVc+/xXrflQTSJHMhPOIpCow08xWAIuA6e4+GXgASDKzVKAiMAbAzHqZ2WPB2A7A8uD6yofAoOAurSrAHDNbDiwEPnH3KWGcgxQyVcsWZ0z/BEZc05zNP+/n0mdmM3zGNxw+qiaQIicTttt/8xLd/itnYue+Qzw2eQ0Tl/3AOVVK83Tf82hWvVykyxLJNRG//Vckv6tYqigjrjmfl29MYM+BI1z+3Fz+/skaDhxWE0iR9BQkIpno0qgK05ISuaZVDV6a/R09RiQzb72aQIqcoCARyYIyxWJ58vKmvH1rawCufWk+D32wkr1qAimiIBE5He3qVmLKPYkMTKzD+EWb6TY0mRlrfop0WSIRpSAROU3Fi0Tz557n8sGg9pQtHsstr6dw9ztL2bnvUKRLE4kIBYnIGWpevRwf39WBIV0a8NmqbXQZOouJy7aqzYoUOlkKEjOra2ZFg/cXmtndJ35sKFKYFYmJ4p4u9fnk7o7UrFiSe8Yt45axKWzbcyDSpYnkmqwekbwPHDOzesBooDrwdtiqEslnGlQpzft3tON/LjmXuet30HVoMm8t2MRxtVmRQiCrQXI86L57OfCMu99P6JfrIhKIjjJu6ViHafd24rz4svzlw1Vc9/J8Nu74NdKliYRVVoPkiJldS6g31uRgWWx4ShLJ32pULMFbt7Tmn1c0ZfXWvXQfnszo5PUcPaY2K1IwZTVIbibUuffv7v6dmdUG3ghfWSL5m5lxTasaTE/qRMf6cTz56ddc8fxXrN22N9KlieS40+61ZWblgeruviI8JeU89dqSSHJ3Plm5jb9NXM2eA0cYdFE97ryoLkVjoiNdmsgp5WivLTP70szKmFkFYAnwkpkNzW6RIoWBmXHpeWczI6kTlzU7m5Gff8ulI+ewZPOuSJcmkiOyemqrrLvvBa4AXnf31kCX8JUlUvCUL1mEYVc359WbWrLv0FGufP4rHp+8hv2Hj2Y+WCQPy2qQxJhZVeAq/nOxXUTOwEUNKzNtSCL9WtdgzJzv6D48mbmpOyJdlsgZy2qQPAZMBda7+yIzqwN8G76yRAq20sVieaJPU8YPbENMVBT9Xl7AAxNWsOeAmkBK/qMHW4lE2MEjxxg+41temr2BiiWL8ESfJnRrfFakyxLJ8Yvt8Wb2oZltD17vm1l89ssUkWKx0Tx4cUM+GtSeiqWKMvCNxdz59hLSflETSMkfsnpq61VgEnB28Po4WHZSZlbMzBaa2XIzW21mjwbLa5vZAjNLNbPxZlYkg7GtzGxZ8FpuZpenW9fDzNYF4x/M6kRF8rqm8WWZNLg993VrwPTVP9F12Cw+XLpFTSAlz8vSqS0zW+buzTNb9pv1BpR0931mFgvMAe4BkoAP3H2cmb0ALHf3538ztgRw2N2PBhf5lxMKMAe+AboCW4BFwLXuvuZU9evUluQ3qdt/4U8TVrBk824uPCeOv1/elGrlike6LClkcvqZ7TvN7Hoziw5e1wOnfNaoh+wLPsYGLwc6AxOC5WOBPhmM3R/09gIoFowDaAWkuvsGdz8MjAN6Z3EOIvlGvcqlee/2dvztskYs2PAz3YbO4o15G9UEUvKkrAbJHwjd+vsjsA3oC9yU2aAgdJYB24HpwHpgd7qQ2AJUO8nY1ma2GlgJ3B6MqQZ8n26zU40faGYpZpaSlpaW+QxF8pjoKOPm9rWZNiSR82uU5+GJq7lm9Hw2pO3LfLBILspSkLj7Jnfv5e5x7l7Z3fsAV2Zh3LHg9Fc8oaOJhlktzN0XuHtjoCXwkJkVy+rYYPxod09w94S4uLjTGSqSp1SvUII3BrTi6b7n8fWPe+kxYjbPf6kmkJJ3ZOcJiUlZ3dDddwMzCTV+LGdmMcGqeGBrJmPXAvuAJsG21dOtznS8SEFgZlyVUJ0ZSZ246Jw4npryNX2em8uaH9QEUiIvO0Fip1xpFnfiKYpmVpzQBfK1hAKlb7BZf2BiBmNrnwgbM6tJ6EhmI6GL6/WD9UWAawjdTSZSKFQuU4wXb0jg+X4t+HHPIXo9O4f/N3UdB48ci3RpUohlJ0gyu+pXFZhpZisIBcB0d58MPAAkmVkqUBEYA2BmvczssWBsB2B5cH3lQ2CQu+8IrpMMJvQr+7XAu+6+OhtzEMmXLm5alRlJifRuXo1nZ6ZyycjZLN70c6TLkkLqlLf/mtkvZBwYBhR395gM1uU5uv1XCrJZ36Tx5w9W8sOeA/RvW4v7u59DyaL54q+m5HE5cvuvu5d29zIZvErnlxARKeg6NYhj6pBEbmxTk7HzNtJtWDLJ3+hORck92Tm1JSJ5RKmiMTzauwnv3taWorFR3PjKQu57bzl79qsJpISfgkSkAGlZqwKf3t2RQRfW5cOlW+kybBZTVm2LdFlSwClIRAqYYrHR/KlHQybe2Z64UkW5/c0l3PHmYrb/cjDSpUkBpSARKaCaVCvLxMHtub/7OXz+9Xa6Dk1mwmI1gZScpyARKcBio6O486J6fHp3R+pXLsV97y3nxlcW8v3P+yNdmhQgChKRQqBe5VK8e1tbHuvdmCWbdtF9eDKvzf1OTSAlRyhIRAqJqCjjxra1mDokkYRaFXjk4zVc9eI8UrerCaRkj4JEpJCJL1+CsTe35N+/b8a32/fRc8RsRs1M5YiaQMoZUpCIFEJmxpUXxDMjqRNdGlXmX1PX0fvZuazauifSpUk+pCARKcTiShfluX4X8ML1LUjbd4jeo+by1JSv1QRSTouCRETo0aQqM4Z04soW1Xj+y/X0HDGbRRvVBFKyRkEiIgCULRHL032b8eaA1hw+dpzfvzCPv05cxb5DRzMfLIWagkRE/kuH+pWYem8iN7evxRvzN9F9WDJfrtse6bIkD1OQiMj/UbJoDH+7rDETbm9H8SLR3PTqIpLeXcauXw9HujTJgxQkInJSF9Qszyd3d+CuzvWYtOwHug6bxacrt6nNivwXBYmInFLRmGj+2O0cJg3uQNWyxRn01hJuf3Mx2/eqCaSEhC1IzKyYmS00s+VmttrMHg2W1zazBWaWambjg2ev/3ZsVzNbbGYrgz87p1v3pZmtM7NlwatyuOYgIv/R6OwyfDioHQ9d3JAv16Xxu6GzeHfR9zo6kbAekRwCOrt7M6A50MPM2gBPAcPcvR6wCxiQwdgdwGXu3hToD7zxm/X93L158NJVQJFcEhMdxW2d6vLZPR05t2oZ/vT+Cm4YoyaQhV3YgsRDTjTxiQ1eDnQGJgTLxwJ9Mhi71N1/CD6uBoqbWdFw1Soip6dOXCnG3dqGJ/o0Ydn3u+k2LJlX5nzHMTWBLJTCeo3EzKLNbBmwHZgOrAd2u/uJG9O3ANUy2c2VwBJ3P5Ru2avBaa2HzcxO8t0DzSzFzFLS0vT8apGcFhVlXN+mJtOGJNK6TgUem7yG37/wFd/+9EukS5NcFtYgcfdj7t4ciAdaAQ1PZ7yZNSZ0Kuy2dIv7Bae8OgavG07y3aPdPcHdE+Li4s6ofhHJ3NnlivPqTS0ZfnVzvtvxK5eMnMMzn3+rJpCFSK7cteXuu4GZQFugnJnFBKviga0ZjTGzeOBD4EZ3X59uX1uDP38B3iYUUCISQWZGn/OrMT2pE90aV+Hf07/hsmfmsGLL7kiXJrkgnHdtxZlZueB9caArsJZQoPQNNusPTMxgbDngE+BBd5+bbnmMmVUK3scClwKrwjUHETk9lUoV5dnrWjD6hgvYtf8wfUbN5R+frlUTyAIunEckVYGZZrYCWARMd/fJwANAkpmlAhWBMQBm1svMHgvGDgbqAX/9zW2+RYGpwT6XETqaeSmMcxCRM9Ct8VlMG9KJq1tW58XkDfQYnsz8DTsjXZaEiRWGe8ATEhI8JSUl0mWIFEpfpe7gwQ9Wsvnn/fRrXYMHL25I6WKxkS5LssDMFrt7Qmbb6ZftIhJW7epVYsq9HbmlQ23eWbiZbsOSmfm1fv5VkChIRCTsShSJ4X8ubcT7d7SjVNEYbn5tEfeOW8rPagJZIChIRCTXnF+jPJPv7sA9v6vPJyu30XXoLD5e/oParORzChIRyVVFY6IZ0rUBH9/VgfjyxbnrnaXc+vpiftyjJpD5lYJERCKi4Vll+GBQe/7S81zmpKbRdegs3lm4WUcn+ZCCREQiJjrKuDXM3Na5AAAP/UlEQVSxDlPuSaRxtTI89MFKrntpAZt2/hrp0uQ0KEhEJOJqVSrJ27e04cnLm7Jq6x66D0/m5dkb1AQyn1CQiEieEBVlXNe6BtOSEmlftxJPfLKWK57/inU/qglkXqcgEZE8pWrZ4rzcP4GR157P9z/v59JnZjN8xjccPqomkHmVgkRE8hwzo1ezs5mR1ImeTasyfMa3XPbMHJZ9ryaQeZGCRETyrAolizDimvMZ0z+BPQeOcMVzc/n7J2s4cFhNIPMSBYmI5Hm/O7cK05ISuaZVDV6a/R3dhyfz1fodkS5LAgoSEckXyhSL5cnLm/LOrW0wg+teWsBDH6xk78EjkS6t0FOQiEi+0rZuRabck8htiXUYv2gzXYfOYsaanyJdVqGmIBGRfKd4kWge6nkuH93ZnvIlinDL6ync9c5Sdu47FOnSCiUFiYjkW+fFl2PS4A4kdW3AlFXb6DJ0FhOXbVWblVymIBGRfK1ITBR3/64+n9zdkZoVS3LPuGUMGJvCD7sPRLq0QkNBIiIFQoMqpXn/jnY8fGkj5q3fSbdhyby1YBPH1WYl7MIWJGZWzMwWmtlyM1ttZo8Gy2ub2QIzSzWz8WZWJIOxXc1ssZmtDP7snG7dBcHyVDMbaWYWrjmISP4SHWUM6FCbqfcm0qx6Wf7y4SqufWk+3+1QE8hwCucRySGgs7s3A5oDPcysDfAUMMzd6wG7gAEZjN0BXObuTYH+wBvp1j0P3ArUD149wjcFEcmPalQswZsDWvPUlU1Zs20vPYYn8+Ks9Rw9pjYr4RC2IPGQfcHH2ODlQGdgQrB8LNAng7FL3f2H4ONqoLiZFTWzqkAZd5/voatpr2c0XkTEzLi6ZQ1mJHUisUEc//jsa654/ivWbtsb6dIKnLBeIzGzaDNbBmwHpgPrgd3ufjTYZAtQLZPdXAkscfdDwbZb0q076XgzG2hmKWaWkpaWlp1piEg+VqVMMUbfcAGjrmvBD7sPcNkzcxg6bR2HjqrNSk4Ja5C4+zF3bw7EA62Ahqcz3swaEzoVdtsZfPdod09w94S4uLjTHS4iBYiZccl5VZk+pBO9mp3NyC9SuXTkHJZs3hXp0gqEXLlry913AzOBtkA5M4sJVsUDWzMaY2bxwIfAje6+Pli8NRhzwknHi4j8VvmSRRh6dXNevbklvx46ypXPf8VjH69h/+GjmQ+WkwrnXVtxZlYueF8c6AqsJRQofYPN+gMTMxhbDvgEeNDd555Y7u7bgL1m1ia4W+vGjMaLiJzKRedUZuqQRK5vXZNX5oaaQM5NVRPIMxXOI5KqwEwzWwEsAqa7+2TgASDJzFKBisAYADPrZWaPBWMHA/WAv5rZsuBVOVg3CHgZSCV0zeWzMM5BRAqo0sViebxPE969rS0xUVH0e3kBD0xYwZ4DagJ5uqwwtBJISEjwlJSUSJchInnUwSPHGPH5t4xO3kDFkkV4vE8Tujc+K9JlRZyZLXb3hMy20y/bRaTQKxYbzQM9GvLRoPZULFWU295YzJ1vLSHtFzWBzAoFiYhIoGl8WSYNbs/93c9h+pqf6DpsFh8s2aImkJlQkIiIpBMbHcWdF9Xj03s6UKdSSZLeXc7Nry1iq5pAnpSCREQkA/Uql+a929vxyGWNWPjdz3QbOos35m1UE8gMKEhERE4iOsq4qX2oCWSLmuV5eOJqrh49j/Vp+zIfXIgoSEREMlG9Qgle/0Mr/tX3PNb9+AsXj5jNc1+mqglkQEEiIpIFZsbvE6oz44+d6HxOZZ6eso4+z81l9Q97Il1axClIREROQ+XSxXjhhgt4vl8LftxziF7PzuVfU7/m4JHC2wRSQSIicgYublqVGUmJ9GlejVEz13PJyNks3vRzpMuKCAWJiMgZKleiCP++qhlj/9CKg0eO0/eFeTwyaTW/HipcTSAVJCIi2dSpQRzThiTSv20txs7bSLdhySR/U3ieg6QgERHJASWLxvBIr8a8d1tbisZGceMrC7nvveXs3n840qWFnYJERCQHJdSqwKd3d+TOi+ry4dKtdBmazGcrt0W6rLBSkIiI5LBisdHc370hkwa3p0qZotzx1hLueHMx2385GOnSwkJBIiISJo3PLstHd7bngR4N+fzr7XQdmsx7Kd8XuCaQChIRkTCKjY7ijgvr8tk9HWlQpRT3T1jBja8s5Puf90e6tByjIBERyQV140oxfmBbHu/dmCWbdtF9eDKvzf2uQDSBVJCIiOSSqCjjhra1mDokkZa1KvDIx2v4/YvzSN3+S6RLy5awBYmZFTOzhWa23MxWm9mjwfLaZrbAzFLNbLyZFclgbEUzm2lm+8zs2d+s+9LM1mXwLHcRkXwhvnwJXru5JUOvasb6tH30HDGHUTNTOZJPm0CG84jkENDZ3ZsBzYEeZtYGeAoY5u71gF3AgAzGHgQeBu47yb77uXvz4LU9DLWLiISVmXFFi3imD+lE10ZV+NfUdfR+di6rtua/JpBhCxIPOdG0PzZ4OdAZmBAsHwv0yWDsr+4+h1CgiIgUWHGlizKqXwteuP4C0vYdoveouTw1JX81gQzrNRIzizazZcB2YDqwHtjt7ica0WwBqp3Brl8NTms9bGZ2ku8eaGYpZpaSllZ4WhWISP7Uo8lZzBjSib4t4nn+y/X0HDGbhd/ljyaQYQ0Sdz/m7s2BeKAV0DAHdtvP3ZsCHYPXDSf57tHunuDuCXFxcTnwtSIi4VW2RCxP9T2PNwe05vCx41z14jwe/mgV+/J4E8hcuWvL3XcDM4G2QDkziwlWxQNbT3NfW4M/fwHeJhRQIiIFRof6lZg2JJE/tK/Nmws20W3oLGauy7uXg8N511acmZUL3hcHugJrCQVK32Cz/sDE09hnjJlVCt7HApcCq3KybhGRvKBEkRj+elkjJtzejhJFY7j51UUkjV/Grl/zXhNIC9dP9c3sPEIX06MJBda77v6YmdUBxgEVgKXA9e5+yMx6AQnu/tdg/EagDFAE2A10AzYByYQu3EcDM4Akdz/lVamEhARPSUnJ+UmKiOSCQ0ePMeqLVJ77cj3lSsTyaK8m9Gx6Fie5RJxjzGyxuydkul1B6/mSEQWJiBQEa7ft5U8TVrBy6x66NarC432aUKVMsbB9X1aDRL9sFxHJJ86tWoYPB7XjoYsbMuubNLoMncX4RZsj3gRSQSIiko/EREdxW6e6TLk3kXOrluGB91dy/ZgFbN4ZuSaQChIRkXyodqWSjLu1DU/0acLy7/fQfXgyY+Z8x7EINIFUkIiI5FNRUcb1bWoybUgibepU4PHJa+j7wld8+1PuNoFUkIiI5HNnlyvOKze1ZMQ1zdm441cuGTmHkZ9/y+GjudMEUkEiIlIAmBm9m1djRlInujc5i6HTv6HXs3P4aW/4WxYqSERECpCKpYryzLXn89KNCdSsWIJKpYqG/TtjMt9ERETym66NqtC1UZVc+S4dkYiISLYoSEREJFsUJCIiki0KEhERyRYFiYiIZIuCREREskVBIiIi2aIgERGRbCkUD7YyszRCT1c8E5WAHTlYTn6gORcOhW3OhW2+kP0513T3uMw2KhRBkh1mlpKVJ4QVJJpz4VDY5lzY5gu5N2ed2hIRkWxRkIiISLYoSDI3OtIFRIDmXDgUtjkXtvlCLs1Z10hERCRbdEQiIiLZoiAREZFsUZAEzKyHma0zs1QzezCD9UXNbHywfoGZ1cr9KnNOFuabZGZrzGyFmX1uZjUjUWdOymzO6ba70szczPL9raJZmbOZXRX8s15tZm/ndo05LQv/btcws5lmtjT497tnJOrMKWb2ipltN7NVJ1lvZjYy+N9jhZm1yPEi3L3Qv4BoYD1QBygCLAca/WabQcALwftrgPGRrjvM870IKBG8vyM/zzercw62Kw0kA/OBhEjXnQv/nOsDS4HywefKka47F+Y8GrgjeN8I2BjpurM550SgBbDqJOt7Ap8BBrQBFuR0DToiCWkFpLr7Bnc/DIwDev9mm97A2OD9BOB3Zma5WGNOynS+7j7T3fcHH+cD8blcY07Lyj9jgMeBp4CDuVlcmGRlzrcCo9x9F4C7b8/lGnNaVubsQJngfVngh1ysL8e5ezLw8yk26Q287iHzgXJmVjUna1CQhFQDvk/3eUuwLMNt3P0osAeomCvV5byszDe9AYT+iyY/y3TOwSF/dXf/JDcLC6Os/HNuADQws7lmNt/MeuRadeGRlTk/AlxvZluAT4G7cqe0iDndv++nLSYndyYFj5ldDyQAnSJdSziZWRQwFLgpwqXkthhCp7cuJHTUmWxmTd19d0SrCq9rgdfc/d9m1hZ4w8yauPvxSBeWX+mIJGQrUD3d5/hgWYbbmFkMoUPinblSXc7Lynwxsy7AX4Be7n4ol2oLl8zmXBpoAnxpZhsJnUuelM8vuGfln/MWYJK7H3H374BvCAVLfpWVOQ8A3gVw93lAMULNDQuqLP19zw4FScgioL6Z1TazIoQupk/6zTaTgP7B+77AFx5cycqHMp2vmZ0PvEgoRPL7eXPIZM7uvsfdK7l7LXevRei6UC93T4lMuTkiK/9ef0ToaAQzq0ToVNeG3Cwyh2VlzpuB3wGY2bmEgiQtV6vMXZOAG4O7t9oAe9x9W05+gU5tEbrmYWaDgamE7vp4xd1Xm9ljQIq7TwLGEDoETiV0YeuayFWcPVmc77+AUsB7wT0Fm929V8SKzqYszrlAyeKcpwLdzGwNcAy4393z65F2Vuf8R+AlMxtC6ML7Tfn4Pwoxs3cI/cdApeC6z9+AWAB3f4HQdaCeQCqwH7g5x2vIx//7iYhIHqBTWyIiki0KEhERyRYFiYiIZIuCREREskVBIiIi2aIgETlDZnbMzJale520o/AZ7LvWybq5iuQ1+h2JyJk74O7NI12ESKTpiEQkh5nZRjN72sxWmtlCM6sXLK9lZl+ke8ZLjWB5FTP70MyWB692wa6izeyl4Dkh08yseLD93emeFTMuQtMU+V8KEpEzV/w3p7auTrduj7s3BZ4FhgfLngHGuvt5wFvAyGD5SGCWuzcj9FyJ1cHy+oRavDcGdgNXBssfBM4P9nN7uCYnklX6ZbvIGTKzfe5eKoPlG4HO7r7BzGKBH929opntAKq6+5Fg+TZ3r2RmaUB8+saYFnoC53R3rx98fgCIdfcnzGwKsI9Qn6yP3H1fmKcqcko6IhEJDz/J+9ORvuPyMf5zTfMSYBSho5dFQTdqkYhRkIiEx9Xp/pwXvP+K/zT77AfMDt5/TuhxxphZtJmVPdlOg+emVHf3mcADhB5n8H+OikRyk/5LRuTMFTezZek+T3H3E7cAlzezFYSOKq4Nlt0FvGpm9xNqW36iC+s9wGgzG0DoyOMO4GRtvqOBN4OwMWBkAX8IleQDukYiksOCayQJ7r4j0rWI5Aad2hIRkWzREYmIiGSLjkhERCRbFCQiIpItChIREckWBYmIiGSLgkRERLLl/wOetahHNmSA3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def eval_step(img_tensor, target):\n",
    "#     \"\"\"\n",
    "#     basically same as train_step, but doesn't apply gradient \n",
    "#     \"\"\"\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     # initializing the hidden state for each batch\n",
    "#     # because the captions are not related from image to image\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']] * target.shape[0], 1)\n",
    "    \n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         features = encoder(img_tensor)\n",
    "\n",
    "#         for i in range(1, target.shape[1]):\n",
    "            \n",
    "#             # passing the features through the decoder\n",
    "#             predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "\n",
    "#             loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "#             # using teacher forcing\n",
    "#             decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "#     total_loss = (loss / int(target.shape[1]))\n",
    "    \n",
    "#     return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Customized to enable multiple images, and incorporate PPLM framework\n",
    "\n",
    "\n",
    "# def custom_evaluate(image, supporting_images):\n",
    "    \n",
    "#     attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "#     # initializing the hidden state for decoder\n",
    "#     hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     # Extract image features\n",
    "#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "#     features = encoder(img_tensor_val)\n",
    "    \n",
    "    \n",
    "#     ## TODO: extract features from supporting images\n",
    "#     ## TODO: average those features into single context vector\n",
    "    \n",
    "    \n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     ## TODO: BERT doesn't have word_index\n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']], 0)\n",
    "#     result = []\n",
    "\n",
    "#     for i in range(max_length):\n",
    "#         \"\"\"\n",
    "#         Currently using inject method \n",
    "#         (image feature injected at every timestep)\n",
    "        \n",
    "#         TODO: test merge approach & pre, par inject\n",
    "#         \"\"\"\n",
    "        \n",
    "#         ## image feature inject's method\n",
    "#         predictions, hidden, attention_weights = decoder(decoder_input, features, hidden)\n",
    "        \n",
    "        \n",
    "#         ## TODO : apply PPLM here\n",
    "#         ## check loss (prediction - context vector of supporting images)\n",
    "#         ## apply gradient : hidden_state += diffrence(pred, supporting img vectors) (after n-iteration)\n",
    "#         ## re compute predictions\n",
    "        \n",
    "        \n",
    "#         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        \n",
    "        \n",
    "#         ## TODO: Allow using BERT tokenizer\n",
    "#         ## TODO: Revert predictions to index and words\n",
    "#         result.append(tokenizer.index_word[predicted_id])   # <<< return back id to text\n",
    "\n",
    "        \n",
    "#         if tokenizer.index_word[predicted_id] == '[END]':\n",
    "#             return result, attention_plot\n",
    "\n",
    "#         decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result), :]\n",
    "#     return result, attention_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
