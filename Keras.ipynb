{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = '../Dataset/MSCOCO/annotations/'\n",
    "image_folder = '../Dataset/MSCOCO/train2014/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']=\"http://jessin:77332066@cache.itb.ac.id:8080\"\n",
    "os.environ['https_proxy']=\"https://jessin:77332066@cache.itb.ac.id:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = annotation_folder + 'captions_train2014.json'\n",
    "\n",
    "# Read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 150\n",
    "\n",
    "\n",
    "# Store captions and image names\n",
    "all_captions = []\n",
    "all_img_paths = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = \"START \" + annot['caption']\n",
    "    image_id = annot['image_id']\n",
    "    img_path = image_folder + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_paths.append(img_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "all_captions, all_img_paths = shuffle(all_captions, all_img_paths, random_state=1)\n",
    "train_captions = all_captions[:NUM_SAMPLES]\n",
    "train_img_paths = all_img_paths[:NUM_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train_captions : 150\n",
      "len all_captions : 414113\n"
     ]
    }
   ],
   "source": [
    "print(\"len train_captions :\", len(train_img_paths))\n",
    "print(\"len all_captions :\", len(all_img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"xception\"\n",
    "\n",
    "\n",
    "def get_encoder(model_type=MODEL_TYPE):\n",
    "\n",
    "    if model_type == \"xception\":\n",
    "        cnn_preprocessor = tf.keras.applications.xception\n",
    "        cnn_model = tf.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "    elif model_type == \"inception_v3\":\n",
    "        cnn_preprocessor = tf.keras.applications.inception_v3\n",
    "        cnn_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    input_layer = cnn_model.input\n",
    "    output_layer = cnn_model.layers[-1].output # use last hidden layer as output\n",
    "    \n",
    "    encoder = tf.keras.Model(input_layer, output_layer)\n",
    "    encoder_preprocessor = cnn_preprocessor\n",
    "    \n",
    "    return encoder, encoder_preprocessor\n",
    "\n",
    "\n",
    "encoder, encoder_preprocessor = get_encoder(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = encoder_preprocessor.preprocess_input(image)\n",
    "    \n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# Get unique images\n",
    "unique_train_img_paths = sorted(set(train_img_paths))\n",
    "\n",
    "# Prepare dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(unique_train_img_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) # use max num of CPU\n",
    "image_dataset = image_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_batch_count 10.375\n"
     ]
    }
   ],
   "source": [
    "estimated_batch_count = NUM_SAMPLES / BATCH_SIZE + 1\n",
    "print(\"estimated_batch_count\", estimated_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:05,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed image (batch)\n",
    "for batch_imgs, batch_img_paths in tqdm(image_dataset):\n",
    "    \n",
    "    # get context vector of batch images\n",
    "    batch_features = encoder(batch_imgs)\n",
    "    \n",
    "    # flatten 2D cnn result into 1D for RNN decoder input\n",
    "    # (batch_size, 10, 10, 2048)  => (batch_size, 100, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    # Cache preprocessed image\n",
    "    for image_feature, image_path in zip(batch_features, batch_img_paths):\n",
    "        image_path = image_path.numpy().decode(\"utf-8\")\n",
    "        np.save(image_path, image_feature.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = \"BERT\"\n",
    "VOCAB_SIZE = 5000  # Choose the top-n words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper(BertTokenizer):\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        convert batch texts into indexed version\n",
    "        eg: ['an apple', 'two person']\n",
    "        output: [[1037,17260], [2083, 2711]] \n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized_texts = [self.tokenize(x) for x in texts]\n",
    "        token_ids = [self.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        \n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper(Tokenizer):\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.word_index[x] for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOKENIZER == \"BERT\" :\n",
    "\n",
    "    # Load pre-trained BERT tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizerWrapper.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "else : \n",
    "    \n",
    "    # use default keras tokenizer\n",
    "    tokenizer = TokenizerWrapper(num_words=VOCAB_SIZE, oov_token=\"[UNK]\")\n",
    "    tokenizer.fit_on_texts(train_captions)    \n",
    "    tokenizer.word_index['[PAD]'] = 0\n",
    "    tokenizer.index_word[0] = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "MAX_LENGTH = None  # use <int> or None\n",
    "\n",
    "\n",
    "train_captions = pad_sequences(train_captions, maxlen=MAX_LENGTH, padding='post', truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def load_image_npy(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "from tensorflow.data import Dataset\n",
    "dataset = Dataset.from_tensor_slices((train_img_paths, train_captions))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "# wrap function into numpy function\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          load_image_npy, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # approx\n",
    "\n",
    "n_batch = int(NUM_SAMPLES / BATCH_SIZE) + 1\n",
    "n_train = int(n_batch * 0.7)\n",
    "n_eval = int(n_batch * 0.15)\n",
    "n_test = n_batch - (n_train + n_eval)\n",
    "\n",
    "train_dataset = dataset.take(n_train)\n",
    "eval_dataset = dataset.skip(n_train).take(n_eval)\n",
    "test_dataset = dataset.skip(n_train + n_eval)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# dataset => tuple of (image, captions)\n",
    "# image   => (batch_size = 16, 100, 2048)\n",
    "# caption => (batch_size = 16, max_length)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 7 batches, (total : 112)\n",
      "eval : 1 batches, (total : 16)\n",
      "test : 2 batches, (total : 32 (aprx))\n"
     ]
    }
   ],
   "source": [
    "print(\"train: {} batches, (total : {})\".format(n_train, n_train * BATCH_SIZE))\n",
    "print(\"eval : {} batches, (total : {})\".format(n_eval, n_eval * BATCH_SIZE))\n",
    "print(\"test : {} batches, (total : {} (aprx))\".format(n_test, n_test * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    \n",
    "    # Image features are extracted and saved already\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "\n",
    "    def __init__(self, output_dim=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = Dense(output_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "        \"\"\"\n",
    "        return => (batch_size, 64, output_dim)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        \n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        features (CNN_encoder output) => (batch_size, 64, embedding_dim)\n",
    "        hidden                        => (batch_size, hidden_size = ??)\n",
    "        \n",
    "        hidden_with_time_axis => (batch_size, 1, hidden_size)\n",
    "        score                 => (batch_size, 64, hidden_size)\n",
    "        attention_weights     => (batch_size, 64, 1)\n",
    "        context_vector (after sum) => (batch_size, hidden_size)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "optimizer = Adam()\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    real => (batch_size = 16, (1))\n",
    "    pred => (batch_size = 16, vocab_size)\n",
    "    mask => Tensor(\"Cast:0\", shape=(batch_size,), dtype=float32)\n",
    "    loss_ => Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(batch_size,), dtype=float32)\n",
    "\n",
    "    return => Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "\n",
    "WORD_EMBEDDING = \"BERT\"\n",
    "RNN_TYPE = \"LSTM\"\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, rnn_type=\"LSTM\", rnn_units=32, \n",
    "                 embedding_type=\"BERT\", embedding_dim=256, \n",
    "                 combine_strategy=\"merge\", combine_layer=\"concat\",\n",
    "                 vocab_size=5000):\n",
    "        \n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.rnn_units = rnn_units\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_type = embedding_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # when to use context_vector [\"inject_init\", \"inject_pre\", \"inject_par\", \"merge\"]\n",
    "        self.combine_strategy = combine_strategy\n",
    "        \n",
    "        # how to use context_vector [\"add\", \"concat\"]\n",
    "        self.combine_layer = combine_layer\n",
    "        \n",
    "        # =====================================================\n",
    "        \n",
    "        self._init_embedding()\n",
    "        self._init_rnn()\n",
    "           \n",
    "        # dense layer to choose word to generate\n",
    "        self.fc1 = Dense(self.rnn_units)\n",
    "        self.fc2 = Dense(vocab_size) # same size as vocab\n",
    "\n",
    "        # attention layer for image (CNN encoder output)\n",
    "        self.attention = BahdanauAttention(self.rnn_units)\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self):\n",
    "        \n",
    "        # embedding layer (process tokenized caption into vector)\n",
    "        if self.embedding_type == \"BERT\":\n",
    "            \n",
    "            self.bert_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.bert_embedding.to('cuda')\n",
    "            \n",
    "            self.embedding_dim = self.bert_embedding.config.hidden_size\n",
    "            self.vocab_size = self.bert_embedding.config.vocab_size\n",
    "            \n",
    "        else:\n",
    "            self.default_embedding = Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        \n",
    "    def _init_rnn(self):\n",
    "        \n",
    "        # rnn layer for captions sequence and/or image's context vector'\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            self.lstm = LSTM(self.rnn_units,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            self.gru = GRU(self.rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           return_state=True,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        \n",
    "    def embedding(self, x):\n",
    "        \"\"\"\n",
    "        Get BERT's embedding for text tokens\n",
    "        \n",
    "        x (Text tokens) => (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.embedding_type == \"BERT\": \n",
    "            return self._bert_embedding(x)\n",
    "        else:\n",
    "            return self._default_embedding(x)\n",
    "        \n",
    "        \n",
    "    def _bert_embedding(self, x, output_layer=11):\n",
    "\n",
    "        # Format as torch Tensor\n",
    "        x = torch.as_tensor(x.numpy())\n",
    "        x = x.type(torch.LongTensor).to('cuda')\n",
    "        \n",
    "        # BERT's embedding\n",
    "        with torch.no_grad():\n",
    "            embedding , _ = self.bert_embedding(x)\n",
    "\n",
    "        # Revert back to tf.Tensor\n",
    "        x = embedding[output_layer].cpu().numpy()\n",
    "        x = tf.convert_to_tensor(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _default_embedding(self, x):\n",
    "        return self.default_embedding(x)\n",
    "    \n",
    "    \n",
    "    def apply_strategy(self, x, context_vector, curr_iter=0):\n",
    "        \"\"\"\n",
    "        context_vector : image's vector\n",
    "        x              : rnn input (word embedding)\n",
    "        strategy       : \n",
    "        curr_iter      : current iteration number\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.combine_strategy == \"inject_init\":\n",
    "            initial_state = tf.squeeze(context_vector) if curr_iter == 1 else None\n",
    "            output, state = self.rnn_model(x, initial_state=initial_state)  \n",
    "            \n",
    "        elif self.combine_strategy == \"inject_pre\":\n",
    "            x = context_vector if curr_iter == 1 else x\n",
    "            output, state = self.rnn_model(x)  \n",
    "            \n",
    "        elif self.combine_strategy == \"inject_par\":\n",
    "            x = self.custom_combine_layer(context_vector, x)\n",
    "            output, state = self.rnn_model(x)              \n",
    "\n",
    "        else: # merge (as default)\n",
    "            output, state = self.rnn_model(x)           \n",
    "            output = self.custom_combine_layer(context_vector, output)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    def rnn_model(self, x, initial_state=None):\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            \n",
    "            # adjust initial state, LSTM has 2 hidden states (h and c)\n",
    "            if initial_state is not None:\n",
    "                init_h = initial_state\n",
    "                init_c = tf.zeros(initial_state.shape)\n",
    "                initial_state = [init_h, init_c]\n",
    "            \n",
    "            output, h_state, c_state = self.lstm(x, initial_state=initial_state)\n",
    "            \n",
    "        elif self.rnn_type == \"GRU\":\n",
    "            output, h_state = self.gru(x, initial_state=initial_state)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('RNN type not supported yet (LSTM / GRU only)')\n",
    "        \n",
    "        return output, h_state\n",
    "    \n",
    "    \n",
    "    def custom_combine_layer(self, x, y):\n",
    "        if self.combine_layer == \"add\":\n",
    "            return self._add_layer(x, y)\n",
    "        else:\n",
    "            return self._concat_layer(x, y)\n",
    "\n",
    "        \n",
    "    def _add_layer(self, x, y):\n",
    "        \n",
    "        if x.shape[1] != y.shape[1] :\n",
    "            exception = \"Cannot combine using 'add' strategy, both tensor has different shape {} & {}\"\n",
    "            raise Exception(exception.format(x.shape, y.shape))\n",
    "            \n",
    "        return tf.keras.layers.add([x, y])\n",
    "            \n",
    "        \n",
    "    def _concat_layer(self, x, y):\n",
    "        return tf.concat([x, y], axis=-1)\n",
    "        \n",
    "    \n",
    "    def call(self, x, context_vector, iteration):\n",
    "        \"\"\" \n",
    "\n",
    "        x : decoder input, also last word generated => (batch_size, 1)\n",
    "        context_vector : image's vector\n",
    "        \"\"\"\n",
    "\n",
    "        # x1 => (batch_size, 1, embedding_dim)\n",
    "        x1 = self.embedding(x)\n",
    "        \n",
    "        x2, rnn_state = self.apply_strategy(x1, context_vector, iteration)\n",
    "            \n",
    "        ## ============================================\n",
    "        ## TODO: add another attention layer ? \n",
    "        ## ============================================\n",
    "            \n",
    "        # x3 shape => (batch_size, 1, rnn_units = 32)\n",
    "        x3 = self.fc1(x2)\n",
    "\n",
    "        # x4 => (batch_size, rnn_units = 32)\n",
    "        x4 = tf.reshape(x3, (-1, x3.shape[2]))\n",
    "\n",
    "        # word_predictions => (batch_size, vocab)\n",
    "        word_predictions = self.fc2(x4)\n",
    "        \n",
    "        ## ====================================================\n",
    "        ## TODO: add PPLM framework here\n",
    "        ## feed-forward, find gradient, apply gradient, feed-forward again \n",
    "        ## ====================================================\n",
    "        \n",
    "        return word_predictions, rnn_state\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.rnn_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "\n",
    "\n",
    "def train_step(img_tensor, target):\n",
    "    \n",
    "    loss = 0\n",
    "    batch_size = target.shape[0]\n",
    "    \n",
    "    ## Get image context vector\n",
    "    features = encoder(img_tensor)\n",
    "    \n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    ## decoder_input == last word generated\n",
    "    decoder_input = tf.expand_dims(target[:, 0], 1)\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        features = encoder(img_tensor)\n",
    "        \n",
    "        for i in range(1, target.shape[1]):\n",
    "\n",
    "            context_vector, attention_weights = attention(features, hidden)\n",
    "            context_vector = tf.expand_dims(context_vector, 1)     \n",
    "            \n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden = decoder(decoder_input, context_vector, iteration=i)\n",
    "            \n",
    "            # loss => Tensor(\"add:0\", shape=(), dtype=float32)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "            \n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables + attention.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    predictions => (batch_size, vocab_size)\n",
    "    decoder_input => tf.Tensor: id=11841, shape=(batch_size, 1), dtype=int32\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 32\n",
    "IMAGE_FEATURE_DIM = 256  # default : 256\n",
    "\n",
    "# will get ignored if using BERT\n",
    "WORD_EMBEDDING_DIM = 256\n",
    "\n",
    "\n",
    "encoder = CNN_Encoder(\n",
    "    output_dim=IMAGE_FEATURE_DIM\n",
    ")\n",
    "\n",
    "attention = BahdanauAttention(units=UNITS)\n",
    "\n",
    "decoder = RNN_Decoder(\n",
    "    rnn_type=\"LSTM\",\n",
    "    rnn_units=UNITS,\n",
    "    embedding_type=\"BERT\",\n",
    "    embedding_dim=WORD_EMBEDDING_DIM,  \n",
    "    combine_strategy=\"merge\", \n",
    "    combine_layer=\"concat\",\n",
    "    vocab_size=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Requirements\n",
    "\n",
    "# combine_strategy = \"inject_init\" : IMAGE_FEATURE_DIM == UNITS\n",
    "# combine_strategy = \"inject_pre\"  : IMAGE_FEATURE_DIM == WORD_EMBEDDING_DIM\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:05,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset)):\n",
    "\n",
    "    batch_loss, t_loss = train_step(img_tensor, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    \n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.4265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:06,  1.03it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 26.618484\n",
      "Time taken for 1 epoch 6.9659013748168945 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 2.4785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:06,  1.06it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 18.314724\n",
      "Time taken for 1 epoch 6.632096767425537 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 0 Loss 2.4326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:06,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 17.226204\n",
      "Time taken for 1 epoch 6.625948429107666 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(train_dataset)):\n",
    "        \n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            \n",
    "        \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def eval_step(img_tensor, target):\n",
    "#     \"\"\"\n",
    "#     basically same as train_step, but doesn't apply gradient \n",
    "#     \"\"\"\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     # initializing the hidden state for each batch\n",
    "#     # because the captions are not related from image to image\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']] * target.shape[0], 1)\n",
    "    \n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         features = encoder(img_tensor)\n",
    "\n",
    "#         for i in range(1, target.shape[1]):\n",
    "            \n",
    "#             # passing the features through the decoder\n",
    "#             predictions, hidden, _ = decoder(decoder_input, features, hidden)\n",
    "\n",
    "#             loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "#             # using teacher forcing\n",
    "#             decoder_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "#     total_loss = (loss / int(target.shape[1]))\n",
    "    \n",
    "#     return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256# ## Customized to enable multiple images, and incorporate PPLM framework\n",
    "\n",
    "\n",
    "# def custom_evaluate(image, supporting_images):\n",
    "    \n",
    "#     attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "#     # initializing the hidden state for decoder\n",
    "#     hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     # Extract image features\n",
    "#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "#     features = encoder(img_tensor_val)\n",
    "    \n",
    "    \n",
    "#     ## TODO: extract features from supporting images\n",
    "#     ## TODO: average those features into single context vector\n",
    "    \n",
    "    \n",
    "#     ## TODO: allow using diffrent strategy for inititalizer ? \n",
    "#     ## TODO: BERT doesn't have word_index\n",
    "#     decoder_input = tf.expand_dims([tokenizer.word_index['[START]']], 0)\n",
    "#     result = []\n",
    "\n",
    "#     for i in range(max_length):\n",
    "#         \"\"\"\n",
    "#         Currently using inject method \n",
    "#         (image feature injected at every timestep)\n",
    "        \n",
    "#         TODO: test merge approach & pre, par inject\n",
    "#         \"\"\"\n",
    "        \n",
    "#         ## image feature inject's method\n",
    "#         predictions, hidden, attention_weights = decoder(decoder_input, features, hidden)\n",
    "        \n",
    "        \n",
    "#         ## TODO : apply PPLM here\n",
    "#         ## check loss (prediction - context vector of supporting images)\n",
    "#         ## apply gradient : hidden_state += diffrence(pred, supporting img vectors) (after n-iteration)\n",
    "#         ## re compute predictions\n",
    "        \n",
    "        \n",
    "#         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        \n",
    "        \n",
    "#         ## TODO: Allow using BERT tokenizer\n",
    "#         ## TODO: Revert predictions to index and words\n",
    "#         result.append(tokenizer.index_word[predicted_id])   # <<< return back id to text\n",
    "\n",
    "        \n",
    "#         if tokenizer.index_word[predicted_id] == '[END]':\n",
    "#             return result, attention_plot\n",
    "\n",
    "#         decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result), :]\n",
    "#     return result, attention_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
